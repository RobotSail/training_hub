{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB Multi-Phase Training with OSFT\n",
        "\n",
        "This notebook demonstrates how to perform LAB (Large-scale Alignment for chatBots) multi-phase training using the OSFT algorithm. The key innovation is that **OSFT eliminates the need for replay buffers** while still preserving all capabilities across training phases.\n",
        "\n",
        "## The Two-Phase LAB Process with OSFT:\n",
        "\n",
        "1. **Phase 1 - Knowledge Tuning (Phase07)**: Training on knowledge-heavy data to build foundational understanding\n",
        "2. **Phase 2 - Skills Training (Phase10)**: Training on skills data with a **reduced unfreeze_rank_ratio** to preserve Phase 1 knowledge\n",
        "\n",
        "## Key Advantages of LAB with OSFT:\n",
        "\n",
        "- ✅ **No replay buffers needed** - OSFT naturally preserves prior knowledge\n",
        "- ✅ **Simpler data pipeline** - Just use your knowledge and skills data directly\n",
        "- ✅ **Better preservation** - Reduce unfreeze_rank_ratio during skills training for optimal retention\n",
        "- ✅ **No catastrophic forgetting** - Built into the algorithm\n",
        "\n",
        "## The Unfreeze Ratio Strategy:\n",
        "\n",
        "- **Phase 1**: Use standard ratio (e.g., 0.3) for knowledge acquisition\n",
        "- **Phase 2**: Reduce by ~10% (e.g., 0.2) to preserve knowledge while adding skills\n",
        "\n",
        "This progressive reduction ensures each phase builds upon the previous without overwriting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's import the necessary libraries and set up our training environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import training_hub for OSFT training\n",
        "from training_hub import osft\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "from io import StringIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging Configuration\n",
        "\n",
        "Set up logging to track progress while preventing notebook crashes from excessive output.\n",
        "\n",
        "**Note:** For production workflows or long-running jobs, we recommend using the script version at `scripts/lab_multiphase_osft_training.py` for better logging consistency and resumption capabilities.\n",
        "\n",
        "**Quick script usage:**\n",
        "```bash\n",
        "python scripts/lab_multiphase_osft_training.py \\\n",
        "  --base-model-path /path/to/model \\\n",
        "  --phase07-data-path /path/to/knowledge.jsonl \\\n",
        "  --phase10-data-path /path/to/skills.jsonl \\\n",
        "  --ckpt-output-base-dir /path/to/checkpoints\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging to show only essential information\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Suppress verbose logging from transformers and other libraries\n",
        "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
        "\n",
        "print(\"✅ Logging configured for notebook environment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n",
        "\n",
        "Let's define some helper functions for checkpoint management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def find_most_recent_checkpoint(output_dir):\n",
        "    \"\"\"\n",
        "    Find the most recent checkpoint in the training output directory.\n",
        "    \n",
        "    Args:\n",
        "        output_dir (str): Training output directory containing hf_format/ subdirectory\n",
        "        \n",
        "    Returns:\n",
        "        str: Path to the most recent checkpoint\n",
        "        \n",
        "    Raises:\n",
        "        ValueError: If no checkpoints are found\n",
        "    \"\"\"\n",
        "    # Get all checkpoint directories under hf_format\n",
        "    checkpoint_pattern = os.path.join(output_dir, \"hf_format\", \"samples_*.0\")\n",
        "    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
        "    \n",
        "    if not checkpoint_dirs:\n",
        "        raise ValueError(f\"No checkpoints found in {os.path.join(output_dir, 'hf_format')}\")\n",
        "    \n",
        "    # Find the most recently created checkpoint\n",
        "    most_recent_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
        "    \n",
        "    return most_recent_checkpoint\n",
        "\n",
        "print(\"✅ Checkpoint utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding LAB Data Requirements\n",
        "\n",
        "LAB training traditionally requires carefully curated datasets for each phase:\n",
        "\n",
        "### Phase07 (Knowledge) Data\n",
        "- Focus on factual knowledge, domain expertise, and foundational understanding\n",
        "- Examples: technical documentation, educational content, reference materials\n",
        "- Format: Standard JSONL with messages\n",
        "\n",
        "### Phase10 (Skills) Data  \n",
        "- Focus on task completion, instruction following, and practical applications\n",
        "- Examples: coding tasks, problem-solving, conversational skills\n",
        "- Format: Standard JSONL with messages\n",
        "\n",
        "### The OSFT Advantage\n",
        "**With traditional SFT**, Phase10 would need:\n",
        "- Skills data\n",
        "- \\+ Phase07 knowledge data (replay)\n",
        "- \\+ Base model instruction data (replay)\n",
        "- = Complex data mixing and large datasets\n",
        "\n",
        "**With OSFT**, Phase10 only needs:\n",
        "- Skills data\n",
        "- That's it! OSFT preserves prior knowledge automatically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration: Model and Data Paths\n",
        "\n",
        "Configure your base model and data paths for the two-phase training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL AND DATA CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Base model configuration\n",
        "BASE_MODEL_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"  # Or your preferred base model\n",
        "# BASE_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "# BASE_MODEL_PATH = \"microsoft/Phi-4-mini-instruct\"\n",
        "\n",
        "# Data paths for each phase\n",
        "PHASE07_DATA_PATH = \"/path/to/your/phase07_knowledge_data.jsonl\"  # Knowledge data\n",
        "PHASE10_DATA_PATH = \"/path/to/your/phase10_skills_data.jsonl\"     # Skills data ONLY (no replay needed!)\n",
        "\n",
        "# Output configuration\n",
        "CHECKPOINT_BASE_DIR = \"/path/to/checkpoints\"\n",
        "EXPERIMENT_PREFIX = \"lab_osft_experiment\"\n",
        "\n",
        "# Create timestamped experiment directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "experiment_name = f\"{EXPERIMENT_PREFIX}_{timestamp}\"\n",
        "\n",
        "print(\"📋 LAB Multi-Phase Configuration with OSFT\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Base Model: {BASE_MODEL_PATH}\")\n",
        "print(f\"Phase07 Data: {PHASE07_DATA_PATH}\")\n",
        "print(f\"Phase10 Data: {PHASE10_DATA_PATH}\")\n",
        "print(f\"Output Directory: {CHECKPOINT_BASE_DIR}/{experiment_name}\")\n",
        "print()\n",
        "print(\"✨ Key Difference from SFT:\")\n",
        "print(\"  Phase10 only needs skills data - no replay buffers!\")\n",
        "print(\"  OSFT preserves Phase07 knowledge automatically.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OSFT-Specific Parameters: The Progressive Unfreeze Strategy\n",
        "\n",
        "The key to successful multi-phase training with OSFT is progressively reducing the `unfreeze_rank_ratio` in each phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# OSFT PROGRESSIVE UNFREEZE STRATEGY\n",
        "# =============================================================================\n",
        "\n",
        "# Phase07: Initial knowledge acquisition\n",
        "PHASE07_UNFREEZE_RATIO = 0.3  # Standard ratio for knowledge learning\n",
        "\n",
        "# Phase10: Reduced ratio for better preservation\n",
        "UNFREEZE_REDUCTION = 0.1  # Reduce by 10% for each subsequent phase\n",
        "PHASE10_UNFREEZE_RATIO = max(0.1, PHASE07_UNFREEZE_RATIO - UNFREEZE_REDUCTION)\n",
        "\n",
        "print(\"🎯 OSFT Progressive Unfreeze Strategy\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Phase07 (Knowledge): unfreeze_rank_ratio = {PHASE07_UNFREEZE_RATIO}\")\n",
        "print(f\"Phase10 (Skills):    unfreeze_rank_ratio = {PHASE10_UNFREEZE_RATIO}\")\n",
        "print(f\"Reduction:           -{UNFREEZE_REDUCTION} per phase\")\n",
        "print()\n",
        "print(\"📊 Strategy Explanation:\")\n",
        "print(f\"  • Phase07 ({PHASE07_UNFREEZE_RATIO}): More freedom to acquire new knowledge\")\n",
        "print(f\"  • Phase10 ({PHASE10_UNFREEZE_RATIO}): Reduced to preserve Phase07 learning\")\n",
        "print()\n",
        "print(\"💡 Guidelines:\")\n",
        "print(\"  • Start with 0.25-0.35 for Phase07\")\n",
        "print(\"  • Reduce by 0.05-0.15 for each subsequent phase\")\n",
        "print(\"  • Never go below 0.1 (too restrictive)\")\n",
        "print(\"  • Adjust based on your preservation needs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Hyperparameters\n",
        "\n",
        "Configure training parameters for both phases. Note that we can use similar settings for both phases since OSFT handles preservation automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING HYPERPARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# Common parameters for both phases\n",
        "MAX_SEQ_LEN = 8_192                 # Maximum sequence length\n",
        "MAX_TOKENS_PER_GPU = 10_000         # Memory limit per GPU\n",
        "NUM_EPOCHS = 2                      # Training epochs per phase\n",
        "WARMUP_STEPS = 0                    # Warmup for Phase07\n",
        "USE_LIGER = True                    # Enable Liger kernels for efficiency\n",
        "\n",
        "# Phase07 specific parameters\n",
        "PHASE07_BATCH_SIZE = 128            # Batch size for knowledge training\n",
        "PHASE07_LEARNING_RATE = 5e-6        # Use low learning rate for better learning quality\n",
        "\n",
        "# Phase10 specific parameters  \n",
        "PHASE10_BATCH_SIZE = 128            # Can use same batch size (no replay data!)\n",
        "PHASE10_LEARNING_RATE = 5e-6        # Use low learning rate for better learning quality\n",
        "PHASE10_WARMUP_STEPS = 0            # No warmup\n",
        "\n",
        "# Distributed training configuration\n",
        "NPROC_PER_NODE = 8                  # Number of GPUs per node\n",
        "NNODES = 1                          # Number of nodes\n",
        "NODE_RANK = 0                       # Rank of this node\n",
        "RDZV_ID = 47                        # Unique job ID\n",
        "RDZV_ENDPOINT = \"127.0.0.1:29500\"   # Rendezvous endpoint\n",
        "\n",
        "print(\"⚙️  Training Hyperparameters\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Max Sequence Length: {MAX_SEQ_LEN:,}\")\n",
        "print(f\"Max Tokens per GPU: {MAX_TOKENS_PER_GPU:,}\")\n",
        "print(f\"Epochs per Phase: {NUM_EPOCHS}\")\n",
        "print()\n",
        "print(\"Phase07 (Knowledge):\")\n",
        "print(f\"  • Batch Size: {PHASE07_BATCH_SIZE}\")\n",
        "print(f\"  • Learning Rate: {PHASE07_LEARNING_RATE}\")\n",
        "print(f\"  • Warmup Steps: {WARMUP_STEPS}\")\n",
        "print()\n",
        "print(\"Phase10 (Skills):\")\n",
        "print(f\"  • Batch Size: {PHASE10_BATCH_SIZE}\")\n",
        "print(f\"  • Learning Rate: {PHASE10_LEARNING_RATE} (reduced for preservation)\")\n",
        "print(f\"  • Warmup Steps: {PHASE10_WARMUP_STEPS}\")\n",
        "print()\n",
        "print(f\"Distributed: {NPROC_PER_NODE} GPUs × {NNODES} nodes = {NPROC_PER_NODE * NNODES} total GPUs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Knowledge Training (Phase07)\n",
        "\n",
        "First, we train on knowledge data to build foundational understanding. This phase uses the standard unfreeze_rank_ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PHASE 1 (PHASE07): KNOWLEDGE TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "phase07_output_dir = os.path.join(CHECKPOINT_BASE_DIR, f\"{experiment_name}_phase07\")\n",
        "\n",
        "print(\"📚 Phase 1: Knowledge Training with OSFT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Starting from: {BASE_MODEL_PATH}\")\n",
        "print(f\"Training data: {PHASE07_DATA_PATH}\")\n",
        "print(f\"Output directory: {phase07_output_dir}\")\n",
        "print(f\"Unfreeze ratio: {PHASE07_UNFREEZE_RATIO}\")\n",
        "print()\n",
        "\n",
        "# Capture output to prevent notebook crashes\n",
        "output_buffer = StringIO()\n",
        "error_buffer = StringIO()\n",
        "\n",
        "phase07_start_time = time.time()\n",
        "\n",
        "try:\n",
        "    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n",
        "        # Phase07 OSFT training\n",
        "        phase07_result = osft(\n",
        "            # Model and data\n",
        "            model_path=BASE_MODEL_PATH,\n",
        "            data_path=PHASE07_DATA_PATH,\n",
        "            ckpt_output_dir=phase07_output_dir,\n",
        "            \n",
        "            # OSFT-specific\n",
        "            unfreeze_rank_ratio=PHASE07_UNFREEZE_RATIO,\n",
        "            \n",
        "            # Training parameters\n",
        "            num_epochs=NUM_EPOCHS,\n",
        "            effective_batch_size=PHASE07_BATCH_SIZE,\n",
        "            learning_rate=PHASE07_LEARNING_RATE,\n",
        "            max_seq_len=MAX_SEQ_LEN,\n",
        "            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n",
        "            \n",
        "            # Data processing\n",
        "            data_output_dir=os.path.join(phase07_output_dir, \"data_processing\"),\n",
        "            warmup_steps=WARMUP_STEPS,\n",
        "            \n",
        "            # Optimization\n",
        "            use_liger=USE_LIGER,\n",
        "            seed=42,\n",
        "            lr_scheduler=\"cosine\",\n",
        "            \n",
        "            # Checkpointing\n",
        "            checkpoint_at_epoch=True,\n",
        "            save_final_checkpoint=True,\n",
        "            \n",
        "            # Distributed training\n",
        "            nproc_per_node=NPROC_PER_NODE,\n",
        "            nnodes=NNODES,\n",
        "            node_rank=NODE_RANK,\n",
        "            rdzv_id=RDZV_ID,\n",
        "            rdzv_endpoint=RDZV_ENDPOINT,\n",
        "        )\n",
        "    \n",
        "    phase07_duration = time.time() - phase07_start_time\n",
        "    \n",
        "    print(f\"✅ Phase07 completed successfully in {phase07_duration/3600:.2f} hours!\")\n",
        "    print(f\"📁 Checkpoint saved to: {phase07_output_dir}\")\n",
        "    print()\n",
        "    print(\"📊 Phase07 Achievements:\")\n",
        "    print(\"  • Base model capabilities: ✅ Preserved\")\n",
        "    print(\"  • New knowledge integrated: ✅ Complete\")\n",
        "    print(\"  • Ready for Phase10: ✅ Yes\")\n",
        "    \n",
        "    # Find the most recent checkpoint for Phase10\n",
        "    PHASE07_CHECKPOINT = find_most_recent_checkpoint(phase07_output_dir)\n",
        "    print(f\"📁 Found most recent Phase07 checkpoint: {PHASE07_CHECKPOINT}\")\n",
        "    print(f\"📁 Ready for Phase10 training!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Phase07 training failed: {e}\")\n",
        "    print(\"\\nError details:\")\n",
        "    print(error_buffer.getvalue())\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Skills Training (Phase10)\n",
        "\n",
        "Now we train on skills data with a **reduced unfreeze_rank_ratio** to preserve Phase07 knowledge while adding new capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PHASE 2 (PHASE10): SKILLS TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "phase10_output_dir = os.path.join(CHECKPOINT_BASE_DIR, f\"{experiment_name}_phase10\")\n",
        "\n",
        "print(\"🎯 Phase 2: Skills Training with OSFT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Starting from: {PHASE07_CHECKPOINT} (Phase07 checkpoint)\")\n",
        "print(f\"Training data: {PHASE10_DATA_PATH}\")\n",
        "print(f\"Output directory: {phase10_output_dir}\")\n",
        "print(f\"Unfreeze ratio: {PHASE10_UNFREEZE_RATIO} (reduced from {PHASE07_UNFREEZE_RATIO})\")\n",
        "print()\n",
        "print(\"💡 Key Innovation:\")\n",
        "print(\"  NO replay buffer needed! OSFT preserves Phase07 knowledge automatically.\")\n",
        "print(\"  The reduced unfreeze_rank_ratio ensures better preservation.\")\n",
        "print()\n",
        "\n",
        "# Capture output to prevent notebook crashes\n",
        "output_buffer = StringIO()\n",
        "error_buffer = StringIO()\n",
        "\n",
        "phase10_start_time = time.time()\n",
        "\n",
        "try:\n",
        "    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n",
        "        # Phase10 OSFT training\n",
        "        phase10_result = osft(\n",
        "            # Start from Phase07 checkpoint\n",
        "            model_path=PHASE07_CHECKPOINT,\n",
        "            data_path=PHASE10_DATA_PATH,\n",
        "            ckpt_output_dir=phase10_output_dir,\n",
        "            \n",
        "            # OSFT-specific: REDUCED ratio for preservation\n",
        "            unfreeze_rank_ratio=PHASE10_UNFREEZE_RATIO,\n",
        "            \n",
        "            # Training parameters\n",
        "            num_epochs=NUM_EPOCHS,\n",
        "            effective_batch_size=PHASE10_BATCH_SIZE,\n",
        "            learning_rate=PHASE10_LEARNING_RATE,  # Lower LR for preservation\n",
        "            max_seq_len=MAX_SEQ_LEN,\n",
        "            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n",
        "            \n",
        "            # Data processing\n",
        "            data_output_dir=os.path.join(phase10_output_dir, \"data_processing\"),\n",
        "            warmup_steps=PHASE10_WARMUP_STEPS,  # Shorter warmup\n",
        "            \n",
        "            # Optimization\n",
        "            use_liger=USE_LIGER,\n",
        "            seed=42,  \n",
        "            lr_scheduler=\"cosine\",\n",
        "            \n",
        "            # Checkpointing\n",
        "            checkpoint_at_epoch=True,\n",
        "            save_final_checkpoint=True,\n",
        "            \n",
        "            # Distributed training\n",
        "            nproc_per_node=NPROC_PER_NODE,\n",
        "            nnodes=NNODES,\n",
        "            node_rank=NODE_RANK,\n",
        "            rdzv_id=RDZV_ID + 1,  # Different ID for Phase10\n",
        "            rdzv_endpoint=RDZV_ENDPOINT,\n",
        "        )\n",
        "    \n",
        "    phase10_duration = time.time() - phase10_start_time\n",
        "    \n",
        "    print(f\"✅ Phase10 completed successfully in {phase10_duration/3600:.2f} hours!\")\n",
        "    print(f\"📁 Final checkpoint saved to: {phase10_output_dir}\")\n",
        "    print()\n",
        "    print(\"📊 Phase10 Achievements:\")\n",
        "    print(\"  • Base model capabilities: ✅ Preserved\")\n",
        "    print(\"  • Phase07 knowledge: ✅ Retained\")  \n",
        "    print(\"  • New skills integrated: ✅ Complete\")\n",
        "    \n",
        "    # Find the most recent checkpoint from Phase10 training\n",
        "    FINAL_CHECKPOINT = find_most_recent_checkpoint(phase10_output_dir)\n",
        "    print(f\"📁 Final model checkpoint: {FINAL_CHECKPOINT}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Phase10 training failed: {e}\")\n",
        "    print(\"\\nError details:\")\n",
        "    print(error_buffer.getvalue())\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Analysis and Summary\n",
        "\n",
        "Let's analyze the complete two-phase training results and understand what we've achieved with OSFT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL ANALYSIS AND SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "total_duration = (phase07_duration + phase10_duration) / 3600\n",
        "\n",
        "print(\"🎉 LAB Multi-Phase OSFT Training Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total training time: {total_duration:.2f} hours\")\n",
        "print(f\"Final model: {FINAL_CHECKPOINT}\")\n",
        "print()\n",
        "\n",
        "# Training summary\n",
        "print(\"📊 Training Summary:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Phase 1 (Knowledge - Phase07):\")\n",
        "print(f\"  • Duration: {phase07_duration/3600:.2f} hours\")\n",
        "print(f\"  • Unfreeze ratio: {PHASE07_UNFREEZE_RATIO}\")\n",
        "print(f\"  • Batch size: {PHASE07_BATCH_SIZE}\")\n",
        "print(f\"  • Learning rate: {PHASE07_LEARNING_RATE}\")\n",
        "print(f\"  • Checkpoint: {PHASE07_CHECKPOINT}\")\n",
        "print()\n",
        "print(\"Phase 2 (Skills - Phase10):\")\n",
        "print(f\"  • Duration: {phase10_duration/3600:.2f} hours\")\n",
        "print(f\"  • Unfreeze ratio: {PHASE10_UNFREEZE_RATIO} (reduced by {UNFREEZE_REDUCTION})\")\n",
        "print(f\"  • Batch size: {PHASE10_BATCH_SIZE}\")\n",
        "print(f\"  • Learning rate: {PHASE10_LEARNING_RATE}\")\n",
        "print(f\"  • Checkpoint: {FINAL_CHECKPOINT}\")\n",
        "print()\n",
        "\n",
        "# Model capabilities\n",
        "print(\"🚀 Your Model Now Has:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"1. ✅ Original base model capabilities (preserved)\")\n",
        "print(\"2. ✅ New knowledge from Phase07 (integrated)\")\n",
        "print(\"3. ✅ Task-specific skills from Phase10 (acquired)\")\n",
        "print(\"4. ❌ Catastrophic forgetting (none!)\")\n",
        "print()\n",
        "\n",
        "# How to use the model\n",
        "print(\"💻 Using Your Trained Model:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"```python\")\n",
        "print(\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
        "print(\"\")\n",
        "print(\"# Load your LAB-OSFT trained model\")\n",
        "print(f\"model = AutoModelForCausalLM.from_pretrained('{FINAL_CHECKPOINT}')\")\n",
        "print(f\"tokenizer = AutoTokenizer.from_pretrained('{FINAL_CHECKPOINT}')\")\n",
        "print(\"\")\n",
        "print(\"# The model now excels at:\")\n",
        "print(\"# 1. General instruction following (preserved from base)\")\n",
        "print(\"# 2. Domain knowledge (from Phase07)\")\n",
        "print(\"# 3. Specific skills (from Phase10)\")\n",
        "print(\"\")\n",
        "print(\"# Test it:\")\n",
        "print(\"prompt = 'Your domain-specific question here'\")\n",
        "print(\"inputs = tokenizer(prompt, return_tensors='pt')\")\n",
        "print(\"outputs = model.generate(**inputs, max_new_tokens=200)\")\n",
        "print(\"response = tokenizer.decode(outputs[0], skip_special_tokens=True)\")\n",
        "print(\"print(response)\")\n",
        "print(\"```\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LAB-OSFT vs LAB-SFT: Key Differences\n",
        "\n",
        "Understanding how OSFT simplifies the LAB training process compared to traditional SFT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LAB-OSFT vs LAB-SFT COMPARISON\n",
        "# =============================================================================\n",
        "\n",
        "print(\"📊 LAB-OSFT vs LAB-SFT Comparison\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "print(\"| Aspect                  | LAB-SFT                     | LAB-OSFT                    |\")\n",
        "print(\"|-------------------------|-----------------------------|-----------------------------|\")\n",
        "print(\"| **Phase07 Data**        | Knowledge data only         | Knowledge data only         |\")\n",
        "print(\"| **Phase10 Data**        | Skills + Phase07 replay     | Skills data ONLY ✨         |\")\n",
        "print(\"|                         | + Base model replay         |                             |\")\n",
        "print(\"| **Data Complexity**     | Complex mixing ratios       | Simple, direct              |\")\n",
        "print(\"| **Data Storage**        | 3x larger (with replays)    | 1x (no replays needed)      |\")\n",
        "print(\"| **Preservation Method** | Data replay buffers         | Algorithm (unfreeze ratio)  |\")\n",
        "print(\"| **Configuration**       | Complex replay ratios       | Simple ratio reduction      |\")\n",
        "print(\"| **Forgetting Risk**     | If replay ratios wrong      | Minimal by design           |\")\n",
        "print(\"| **Training Time**       | Longer (more data)          | Shorter (less data)         |\")\n",
        "print(\"| **Memory Usage**        | Similar                     | Similar                     |\")\n",
        "print()\n",
        "print(\"✨ Key OSFT Advantages:\")\n",
        "print(\"  1. No need to store or manage replay buffers\")\n",
        "print(\"  2. Simpler data pipeline - just your new data\")\n",
        "print(\"  3. Progressive unfreeze strategy ensures preservation\")\n",
        "print(\"  4. Reduced training time (less data to process)\")\n",
        "print(\"  5. Guaranteed preservation through algorithm design\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices and Recommendations\n",
        "\n",
        "Key guidelines for successful multi-phase training with OSFT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BEST PRACTICES AND RECOMMENDATIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"📚 Best Practices for LAB Multi-Phase OSFT Training\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"1️⃣  **Unfreeze Ratio Strategy:**\")\n",
        "print(\"   • Start with 0.25-0.35 for Phase07\")\n",
        "print(\"   • Reduce by 0.05-0.15 for Phase10\")\n",
        "print(\"   • Never go below 0.1 (too restrictive)\")\n",
        "print(\"   • If seeing forgetting, reduce the ratio further\")\n",
        "print()\n",
        "print(\"2️⃣  **Data Quality:**\")\n",
        "print(\"   • Phase07: Focus on high-quality knowledge/facts\")\n",
        "print(\"   • Phase10: Focus on diverse skills and tasks\")\n",
        "print(\"   • No need for replay data - OSFT handles preservation!\")\n",
        "print()\n",
        "print(\"3️⃣  **Learning Rate Strategy:**\")\n",
        "print(\"   • Phase07: Standard LR (e.g., 5e-6)\")\n",
        "print(\"   • Phase10: Standard LR (e.g., 5e-6) for stability\")\n",
        "print(\"   • Use cosine scheduler for smooth convergence\")\n",
        "print(\"   • If model isn't learning enough, try increasing LR or number of epochs;\")\n",
        "print(\"     the optimal settings will vary by model and data\")\n",
        "print()\n",
        "print(\"4️⃣  **Monitoring:**\")\n",
        "print(\"   • Track loss curves for both phases\")\n",
        "print(\"   • Test preservation after each phase\")\n",
        "print(\"   • Evaluate on held-out sets for each capability\")\n",
        "\n",
        "print(\"🎯 Next Steps:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"1. Test your model on evaluation sets\")\n",
        "print(\"2. Compare with base model to verify improvements\")\n",
        "print(\"3. Fine-tune unfreeze ratios if needed\")\n",
        "print(\"4. Deploy with confidence - no regression expected!\")\n",
        "print()\n",
        "print(\"📝 For production use, remember to use the script:\")\n",
        "print(\"   scripts/lab_multiphase_osft_training.py\")\n",
        "print()\n",
        "print(\"Happy training! 🚀\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Concepts Explained\n",
        "\n",
        "### LAB Multi-Phase Training with OSFT Benefits:\n",
        "\n",
        "1. **Knowledge → Skills with Progressive Unfreeze**: Phase07 builds foundational knowledge, Phase10 adds task-specific capabilities using reduced unfreeze ratio\n",
        "2. **No Replay Buffers Needed**: OSFT's orthogonal subspace approach preserves prior learning algorithmically\n",
        "3. **Simple Data Pipeline**: Just your knowledge and skills data - no complex mixing or replay datasets\n",
        "4. **Progressive Preservation**: Reducing unfreeze_rank_ratio in each phase ensures cumulative learning\n",
        "5. **Guaranteed Non-Forgetting**: Mathematical guarantee through orthogonal weight updates\n",
        "\n",
        "### OSFT Algorithm Advantages:\n",
        "\n",
        "1. **Algorithmic Preservation**: Prior knowledge preserved through math, not data replay\n",
        "2. **Simplified Training**: No need to manage replay buffers or mixing ratios\n",
        "3. **Reduced Storage**: No need to store ~370k sample replay buffers\n",
        "4. **Streamlined Training**: No replay data processing required\n",
        "5. **Predictable Behavior**: Preservation is guaranteed by the algorithm\n",
        "\n",
        "### LAB-OSFT Training Strategy:\n",
        "\n",
        "- **Phase07 Focus**: Knowledge acquisition with standard unfreeze ratio (0.25-0.35)\n",
        "- **Phase10 Focus**: Skills training with reduced ratio (0.15-0.25) for preservation\n",
        "- **Progressive Reduction**: Each phase reduces unfreeze_rank_ratio by 0.05-0.15\n",
        "- **Memory Management**: Same `max_tokens_per_gpu` strategy as SFT\n",
        "- **Fast Data Loading**: Using `/dev/shm` for data processing\n",
        "- **Simple Checkpointing**: Standard checkpointing, no special requirements\n",
        "\n",
        "### Data Requirements Comparison:\n",
        "\n",
        "**Traditional LAB-SFT**:\n",
        "- Phase07: Knowledge data\n",
        "- Phase10: Skills data + Knowledge replay + ~370k sample replay buffer\n",
        "\n",
        "**LAB-OSFT (Our Approach)**:\n",
        "- Phase07: Knowledge data only  \n",
        "- Phase10: Skills data only\n",
        "- Result: Eliminates the replay buffer requirement entirely!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual comparison of training approaches\n",
        "print(\"📊 Training Approach Comparison\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"Traditional LAB-SFT:\")\n",
        "print(\"  Phase07: [Knowledge Data] → Model_v1\")\n",
        "print(\"  Phase10: [Skills Data] + [Knowledge Replay] + [~370k Replay Buffer] → Model_v2\")\n",
        "print(\"           ~~~~~~~~~~~~~   ^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^\")\n",
        "print(\"                               Requires additional replay buffers\")\n",
        "print()\n",
        "print(\"LAB-OSFT (This Notebook):\")\n",
        "print(\"  Phase07: [Knowledge Data] → Model_v1 (unfreeze_ratio=0.3)\")\n",
        "print(\"  Phase10: [Skills Data] → Model_v2 (unfreeze_ratio=0.2)\")\n",
        "print(\"           ~~~~~~~~~~~~~\")\n",
        "print(\"        No replay buffers needed!\")\n",
        "print()\n",
        "print(\"✨ Key Benefit: Eliminates the ~370k sample replay buffer requirement!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "1. **Out of Memory (OOM)**:\n",
        "   - Reduce `max_tokens_per_gpu`\n",
        "   - Set `use_liger` to True\n",
        "   - Reduce `unfreeze_rank_ratio`\n",
        "   - Check GPU memory usage with `nvidia-smi`\n",
        "\n",
        "2. **Model Not Learning Well**:\n",
        "   - Check if `unfreeze_rank_ratio` is too low (try increasing slightly)\n",
        "   - Verify data quality and format\n",
        "   - Consider increasing learning rate or epochs\n",
        "   - Ensure warmup steps are appropriate for your dataset size\n",
        "\n",
        "3. **Knowledge Forgetting in Phase10**:\n",
        "   - Reduce Phase10's `unfreeze_rank_ratio` further (e.g., from 0.2 to 0.15)\n",
        "   - Consider using a lower learning rate in Phase10\n",
        "   - Verify you're loading the correct Phase07 checkpoint\n",
        "\n",
        "4. **Checkpoint Not Found**:\n",
        "   - Verify Phase07 completed successfully\n",
        "   - Check `ckpt_output_dir` permissions\n",
        "   - Look for error messages in training logs\n",
        "   - Ensure sufficient disk space\n",
        "\n",
        "5. **Distributed Training Issues**:\n",
        "   - Verify network connectivity between nodes\n",
        "   - Check `rdzv_endpoint` accessibility\n",
        "   - Ensure consistent environment across nodes\n",
        "   - Try with single node first to isolate issues\n",
        "\n",
        "6. **Data Loading Errors**:\n",
        "   - Verify JSONL format (each line must be valid JSON with 'messages' field)\n",
        "   - Check file paths and permissions\n",
        "   - Ensure sufficient disk space in `/dev/shm`\n",
        "   - Validate no corrupted entries in data\n",
        "\n",
        "### OSFT-Specific Tips:\n",
        "\n",
        "- **Finding Optimal Unfreeze Ratios**: Start with 0.3 for Phase07, reduce by 0.1 for Phase10. Adjust based on your preservation needs.\n",
        "- **Balancing Learning vs Preservation**: Higher ratios = more learning, lower ratios = more preservation\n",
        "- **Multi-Phase Beyond Two**: For 3+ phases, continue reducing ratio by 0.05-0.1 per phase, but don't go below 0.1\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
