{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive OSFT Training Tutorial\n",
        "\n",
        "This notebook provides a comprehensive guide to Orthogonal Subspace Fine-Tuning (OSFT) using the training_hub library. We'll cover:\n",
        "\n",
        "- **All available parameters** and their detailed explanations\n",
        "- **Single-node and multi-node training** configurations\n",
        "- **Popular model examples** (Qwen 2.5 7B Instruct, Llama 3.1 8B Instruct, Phi 4 Mini, etc.)\n",
        "- **Best practices and troubleshooting**\n",
        "\n",
        "OSFT (Orthogonal Subspace Fine-Tuning) is an algorithm based on [Nayak et al. (2025), arXiv:2504.07097](https://arxiv.org/abs/2504.07097) that enables continual training of pre-trained or instruction-tuned models **without** catastrophic forgetting and **without** needing replay buffers or supplementary datasets.\n",
        "\n",
        "This tutorial serves as both a learning resource and a template you can adapt for your specific continual learning needs.\n",
        "\n",
        "**Note:** For production workflows, we also provide focused example scripts for popular models: `scripts/osft_qwen_example.py`, `scripts/osft_llama_example.py`, and `scripts/osft_phi_example.py` with better logging consistency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is OSFT?\n",
        "\n",
        "OSFT (Orthogonal Subspace Fine-Tuning) is a continual learning algorithm that allows you to adapt pre-trained or instruction-tuned models to new domains **without catastrophic forgetting**. Based on [Nayak et al. (2025), arXiv:2504.07097](https://arxiv.org/abs/2504.07097), OSFT fundamentally changes how we approach model adaptation.\n",
        "\n",
        "### Key Innovation\n",
        "\n",
        "Traditional fine-tuning updates all model parameters, which can overwrite previously learned knowledge. OSFT instead:\n",
        "1. **Identifies orthogonal subspaces** in the model's weight matrices\n",
        "2. **Restricts updates to these subspaces**, preserving existing knowledge\n",
        "3. **Eliminates the need for replay buffers** or supplementary datasets\n",
        "\n",
        "### OSFT vs Traditional Fine-Tuning\n",
        "\n",
        "| Aspect | Traditional SFT | OSFT |\n",
        "|--------|----------------|------|\n",
        "| **Catastrophic Forgetting** | Common problem | Prevented by design |\n",
        "| **Data Requirements** | Needs replay/mixed data | Only new domain data |\n",
        "| **Preservation Method** | Data mixing ratios | Algorithm (math guarantees) |\n",
        "| **Memory Usage** | Similar | Similar |\n",
        "| **Complexity** | Complex data pipelines | Simple, direct |\n",
        "\n",
        "### When to Use OSFT\n",
        "\n",
        "**Perfect for:**\n",
        "- Adding domain-specific knowledge (medical, legal, technical)\n",
        "- Adapting to new languages or dialects\n",
        "- Customizing instruction formats\n",
        "- Continual learning across multiple domains\n",
        "- Any scenario where you need to preserve existing capabilities\n",
        "\n",
        "**Not needed for:**\n",
        "- Training from scratch\n",
        "- Base model pre-training\n",
        "- When you want to completely replace model behavior\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Key Parameter: `unfreeze_rank_ratio`\n",
        "\n",
        "The `unfreeze_rank_ratio` is the most important OSFT-specific parameter. It controls the balance between preservation and adaptation.\n",
        "\n",
        "### What Does It Do?\n",
        "\n",
        "- Controls **how much of each weight matrix** can be updated during training\n",
        "- Range: `0.0` to `1.0`\n",
        "- Lower values = more preservation, slower adaptation\n",
        "- Higher values = more adaptation, slightly less preservation\n",
        "\n",
        "### Visual Intuition\n",
        "\n",
        "Think of a weight matrix as a building:\n",
        "- `unfreeze_rank_ratio = 0.1`: You can only renovate 10% of the rooms\n",
        "- `unfreeze_rank_ratio = 0.3`: You can renovate 30% of the rooms\n",
        "- `unfreeze_rank_ratio = 1.0`: You can renovate the entire building (standard fine-tuning)\n",
        "\n",
        "The \"rooms\" you renovate are carefully chosen to be orthogonal to existing knowledge, preventing damage to what's already there.\n",
        "\n",
        "### Recommended Settings by Use Case\n",
        "\n",
        "| Use Case | Recommended Ratio | Why? |\n",
        "|----------|-------------------|------|\n",
        "| **Minor format adjustments** | 0.1-0.15 | Minimal changes needed |\n",
        "| **Domain vocabulary addition** | 0.15-0.25 | Add terms without losing general knowledge |\n",
        "| **Domain specialization** | 0.25-0.35 | Balance preservation and new expertise |\n",
        "| **Major capability expansion** | 0.35-0.5 | Significant new learning required |\n",
        "| **Complete repurposing** | >0.5 | Rarely needed, approaching standard fine-tuning |\n",
        "\n",
        "### Practical Guidelines\n",
        "\n",
        "```python\n",
        "# Conservative: Maximum preservation\n",
        "unfreeze_rank_ratio = 0.2  # Great for adding specialized knowledge\n",
        "\n",
        "# Balanced: Good for most use cases  \n",
        "unfreeze_rank_ratio = 0.3  # Ideal default for domain adaptation\n",
        "\n",
        "# Aggressive: When you need significant changes\n",
        "unfreeze_rank_ratio = 0.4  # Use when preservation is less critical\n",
        "```\n",
        "\n",
        "**Pro tip:** Start conservative (0.2-0.3) and increase only if needed. It's easier to train again with higher ratio than to recover lost capabilities!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The `target_patterns` Parameter (Advanced Users Only)\n",
        "\n",
        "There's an optional `target_patterns` parameter that allows targeting specific model layers for OSFT:\n",
        "\n",
        "```python\n",
        "target_patterns = None  # Default: applies OSFT to all appropriate layers (RECOMMENDED)\n",
        "```\n",
        "\n",
        "**⚠️ Important:** This is an expert-level parameter. Unless you have deep knowledge of model architecture and a specific reason to limit OSFT to certain layers, **leave it as `None`**.\n",
        "\n",
        "If you do need to use it, it performs simple substring matching on module names:\n",
        "- `target_patterns = [\"attention\"]` → Targets modules with \"attention\" in the name\n",
        "- `target_patterns = [\"mlp\"]` → Targets modules with \"mlp\" in the name\n",
        "\n",
        "**For 99% of users:** Just use the default (`None`) and let OSFT handle layer selection automatically. The algorithm knows what it's doing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import training_hub for OSFT training\n",
        "from training_hub import osft\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Format Requirements\n",
        "\n",
        "Before configuring your training, ensure your data is in the correct format. OSFT uses the mini-trainer backend, which supports both standard messages format and pre-processed datasets.\n",
        "\n",
        "### Required Format: JSONL with Messages\n",
        "\n",
        "Your training data must be a **JSON Lines (.jsonl)** file where each line contains a conversation sample:\n",
        "\n",
        "```json\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello, how are you?\"}, {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you! How can I help you today?\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}, {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"}]}\n",
        "```\n",
        "\n",
        "### Message Structure\n",
        "\n",
        "Each conversation contains a `messages` array with message objects having:\n",
        "- **`role`**: One of `\"system\"`, `\"user\"`, `\"assistant\"`, or `\"pretraining\"`\n",
        "- **`content`**: The text content of the message\n",
        "- **`reasoning_content`** (optional): Additional reasoning traces\n",
        "\n",
        "### Masking Control with `unmask_messages` Parameter\n",
        "\n",
        "Control which parts of the conversation are used for training loss:\n",
        "\n",
        "#### Standard Instruction Tuning (default)\n",
        "```python\n",
        "osft(..., unmask_messages=False)  # Only assistant responses used for loss\n",
        "```\n",
        "- **Trains only on assistant responses** (standard instruction-following)\n",
        "- System messages are always masked (ignored for loss)\n",
        "- User messages are masked\n",
        "- Assistant messages are unmasked (used for loss calculation)\n",
        "\n",
        "#### Pretraining Mode\n",
        "```python\n",
        "osft(..., unmask_messages=True)   # All content except system messages used for loss\n",
        "```\n",
        "- **Trains on all content except system messages**\n",
        "- System messages are always masked\n",
        "- User and assistant messages are both unmasked\n",
        "- Useful for pretraining-style data where the model should learn from all text\n",
        "\n",
        "### Pre-processed Dataset Option\n",
        "\n",
        "If you have pre-processed data with `input_ids` and `labels` fields:\n",
        "\n",
        "```json\n",
        "{\"input_ids\": [1, 2, 3, ...], \"labels\": [1, 2, 3, ...]}\n",
        "```\n",
        "\n",
        "Use with:\n",
        "```python\n",
        "osft(..., use_processed_dataset=True)\n",
        "```\n",
        "\n",
        "### Data Path Configuration\n",
        "\n",
        "When configuring your training, point to your JSONL file:\n",
        "\n",
        "```python\n",
        "data_path = \"/path/to/your/training_data.jsonl\"  # Your messages-format JSONL file\n",
        "```\n",
        "\n",
        "The training pipeline will automatically:\n",
        "1. Load and validate your JSONL data\n",
        "2. Apply chat templates based on your model\n",
        "3. Handle masking according to the `unmask_messages` setting\n",
        "4. Process the data for efficient training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Configuration Examples\n",
        "\n",
        "Here are configuration examples for popular models. These serve as starting points - adjust based on your specific hardware and continual learning requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Example: Qwen 2.5 7B Instruct\n",
            "Model Path: Qwen/Qwen2.5-7B-Instruct\n",
            "OSFT Unfreeze Rank Ratio: 0.25\n",
            "Example Max Tokens per GPU: 10,000\n",
            "Example Max Sequence Length: 8,196\n",
            "Example Batch Size: 128\n",
            "Example Learning Rate: 5e-06\n",
            "Notes: Excellent for domain adaptation while preserving multilingual capabilities\n",
            "\n",
            "💡 Remember: OSFT preserves original capabilities without needing replay buffers!\n",
            "   Adjust unfreeze_rank_ratio based on preservation vs adaptation needs.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# MODEL CONFIGURATION EXAMPLES FOR OSFT\n",
        "# These are example configurations - adjust based on your hardware and requirements\n",
        "# =============================================================================\n",
        "\n",
        "# Example 1: Qwen 2.5 7B Instruct\n",
        "qwen_example = {\n",
        "    \"model_name\": \"Qwen 2.5 7B Instruct\",\n",
        "    \"model_path\": \"Qwen/Qwen2.5-7B-Instruct\",  # HuggingFace model name or local path\n",
        "    \"example_unfreeze_rank_ratio\": 0.25,  # Conservative for preserving multilingual capabilities\n",
        "    \"example_max_tokens_per_gpu\": 10000,\n",
        "    \"example_max_seq_len\": 8196,  # Qwen 2.5 supports long context\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 5e-6, \n",
        "    \"notes\": \"Excellent for domain adaptation while preserving multilingual capabilities\"\n",
        "}\n",
        "\n",
        "# Example 2: Llama 3.1 8B Instruct\n",
        "llama_example = {\n",
        "    \"model_name\": \"Llama 3.1 8B Instruct\",\n",
        "    \"model_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",  # HuggingFace model name or local path\n",
        "    \"example_unfreeze_rank_ratio\": 0.3,  # Slightly higher for more adaptation freedom\n",
        "    \"example_max_tokens_per_gpu\": 10000,\n",
        "    \"example_max_seq_len\": 8192,  # Supports up to 128K but 8K is common\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 5e-6,\n",
        "    \"notes\": \"Ideal for adding specialized knowledge without losing general capabilities\"\n",
        "}\n",
        "\n",
        "# Example 3: Phi 4 Mini\n",
        "phi_example = {\n",
        "    \"model_name\": \"Phi 4 Mini\",\n",
        "    \"model_path\": \"microsoft/Phi-4-mini-instruct\",  # HuggingFace model name or local path\n",
        "    \"example_unfreeze_rank_ratio\": 0.25,  # Conservative for smaller model\n",
        "    \"example_max_tokens_per_gpu\": 8192,\n",
        "    \"example_max_seq_len\": 4096,\n",
        "    \"example_batch_size\": 64,\n",
        "    \"example_learning_rate\": 5e-6,\n",
        "    \"notes\": \"Efficient for edge deployment with continual adaptation\"\n",
        "}\n",
        "\n",
        "# Example 4: Generic 7B Base Model\n",
        "generic_7b_example = {\n",
        "    \"model_name\": \"Generic 7B Base\",\n",
        "    \"model_path\": \"/path/to/your-7b-model\",  # Local path to model directory\n",
        "    \"example_unfreeze_rank_ratio\": 0.3,  # Balanced preservation vs adaptation\n",
        "    \"example_max_tokens_per_gpu\": 10000,\n",
        "    \"example_max_seq_len\": 4096,\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 5e-6,\n",
        "    \"notes\": \"Good baseline for most 7B instruction-tuned models\"\n",
        "}\n",
        "\n",
        "# Example 5: Smaller Model (1B-3B)\n",
        "small_model_example = {\n",
        "    \"model_name\": \"Small Model (1B-3B)\",\n",
        "    \"model_path\": \"/path/to/small-model\",  # Local path or HuggingFace name\n",
        "    \"example_unfreeze_rank_ratio\": 0.4,  # Higher ratio for smaller models\n",
        "    \"example_max_tokens_per_gpu\": 16_000,\n",
        "    \"example_max_seq_len\": 4096,\n",
        "    \"example_batch_size\": 128,\n",
        "    \"example_learning_rate\": 3e-5,\n",
        "    \"notes\": \"Smaller models can handle more aggressive adaptation\"\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# SELECT YOUR CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Choose one of the examples above as a starting point\n",
        "selected_example = qwen_example  # Change this to your preferred example\n",
        "\n",
        "print(f\"Selected Example: {selected_example['model_name']}\")\n",
        "print(f\"Model Path: {selected_example['model_path']}\")\n",
        "print(f\"OSFT Unfreeze Rank Ratio: {selected_example['example_unfreeze_rank_ratio']}\")\n",
        "print(f\"Example Max Tokens per GPU: {selected_example['example_max_tokens_per_gpu']:,}\")\n",
        "print(f\"Example Max Sequence Length: {selected_example['example_max_seq_len']:,}\")\n",
        "print(f\"Example Batch Size: {selected_example['example_batch_size']:,}\")\n",
        "print(f\"Example Learning Rate: {selected_example['example_learning_rate']}\")\n",
        "print(f\"Notes: {selected_example['notes']}\")\n",
        "print(\"\\n💡 Remember: OSFT preserves original capabilities without needing replay buffers!\")\n",
        "print(\"   Adjust unfreeze_rank_ratio based on preservation vs adaptation needs.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Parameter Reference\n",
        "\n",
        "Let's configure all available OSFT parameters with detailed explanations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Required Parameters (all must be specified):\n",
            "  • model_path: Qwen/Qwen2.5-7B-Instruct\n",
            "  • data_path: /mnt/nvme2n1/datasets/tablegpt/train/train_ListExtraction.jsonl\n",
            "  • ckpt_output_dir: /mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_190129\n",
            "  • unfreeze_rank_ratio: 0.25\n",
            "  • effective_batch_size: 128\n",
            "  • max_tokens_per_gpu: 10,000\n",
            "  • max_seq_len: 8,196\n",
            "  • learning_rate: 5e-06\n",
            "\n",
            "🔧 OSFT-Specific Parameters:\n",
            "  unfreeze_rank_ratio: 0.25 - Controls how much of each matrix is unfrozen\n",
            "    • 0.1-0.3: Conservative, maximum preservation\n",
            "    • 0.3-0.5: Balanced adaptation\n",
            "    • >0.5: Rarely needed for typical use cases\n",
            "  target_patterns: None - Optional patterns for selecting specific modules\n",
            "\n",
            "🎯 Training Hyperparameters:\n",
            "  effective_batch_size: 128 - Effective batch size for training\n",
            "  learning_rate: 5e-06 - Learning rate for model updates\n",
            "  num_epochs: 1 - Number of training epochs\n",
            "  lr_scheduler: 'cosine' - Learning rate scheduler type\n",
            "  lr_scheduler_kwargs: {} - Scheduler parameters\n",
            "  warmup_steps: 0 - Number of warmup steps\n",
            "  seed: 42 - Random seed for reproducibility\n",
            "\n",
            "⚡ Memory and Performance Parameters:\n",
            "  max_tokens_per_gpu: 10,000 - Maximum tokens per GPU (hard-cap for memory)\n",
            "  max_seq_len: 8,196 - Maximum sequence length\n",
            "  use_liger: True - Use Liger kernels for efficiency\n",
            "\n",
            "💾 Data Processing Parameters:\n",
            "  data_path: '/mnt/nvme2n1/datasets/tablegpt/train/train_ListExtraction.jsonl' - Path to training data (JSONL format)\n",
            "  data_output_dir: '/dev/shm/osft_data' - Directory to save processed data\n",
            "  use_processed_dataset: False - Whether to use pre-processed data\n",
            "  unmask_messages: False - Whether to unmask all messages\n",
            "\n",
            "💾 Checkpointing Parameters:\n",
            "  ckpt_output_dir: '/mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_190129' - Directory to save checkpoints\n",
            "  checkpoint_at_epoch: True - Whether to checkpoint at each epoch\n",
            "  save_final_checkpoint: True - Whether to save final checkpoint\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE OSFT PARAMETER CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Experiment identification\n",
        "experiment_name = \"osft_comprehensive_example\"\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "full_experiment_name = f\"{experiment_name}_{timestamp}\"\n",
        "\n",
        "# =============================================================================\n",
        "# REQUIRED PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# TODO: revert these overrides after we've concluded training\n",
        "model_path = selected_example[\"model_path\"]  # HuggingFace model name or local path\n",
        "# data_path = \"/path/to/your/training_data.jsonl\"  # Path to training data in JSONL format\n",
        "data_path = \"/mnt/nvme2n1/datasets/tablegpt/train/train_ListExtraction.jsonl\"  # Path to training data in JSONL format\n",
        "# ckpt_output_dir = f\"/path/to/checkpoints/{full_experiment_name}\"  # Where to save checkpoints\n",
        "ckpt_output_dir = f\"/mnt/nvme2n1/checkpoints/{full_experiment_name}\"  # Where to save checkpoints\n",
        "unfreeze_rank_ratio = selected_example[\"example_unfreeze_rank_ratio\"]  # OSFT-specific parameter\n",
        "effective_batch_size = selected_example[\"example_batch_size\"]  # Effective batch size for training\n",
        "max_tokens_per_gpu = selected_example[\"example_max_tokens_per_gpu\"]  # Maximum tokens per GPU (memory limit)\n",
        "max_seq_len = selected_example[\"example_max_seq_len\"]  # Maximum sequence length\n",
        "learning_rate = selected_example[\"example_learning_rate\"]  # Learning rate for training\n",
        "\n",
        "print(\"📋 Required Parameters (all must be specified):\")\n",
        "print(f\"  • model_path: {model_path}\")\n",
        "print(f\"  • data_path: {data_path}\")\n",
        "print(f\"  • ckpt_output_dir: {ckpt_output_dir}\")\n",
        "print(f\"  • unfreeze_rank_ratio: {unfreeze_rank_ratio}\")\n",
        "print(f\"  • effective_batch_size: {effective_batch_size}\")\n",
        "print(f\"  • max_tokens_per_gpu: {max_tokens_per_gpu:,}\")\n",
        "print(f\"  • max_seq_len: {max_seq_len:,}\")\n",
        "print(f\"  • learning_rate: {learning_rate}\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# OSFT-SPECIFIC PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "target_patterns = None  # Optional: Patterns to match specific modules for OSFT\n",
        "# Example: [\"*attention*\", \"*mlp*\"] to target attention and MLP layers\n",
        "\n",
        "print(\"🔧 OSFT-Specific Parameters:\")\n",
        "print(f\"  unfreeze_rank_ratio: {unfreeze_rank_ratio} - Controls how much of each matrix is unfrozen\")\n",
        "print(f\"    • 0.1-0.3: Conservative, maximum preservation\")\n",
        "print(f\"    • 0.3-0.5: Balanced adaptation\")\n",
        "print(f\"    • >0.5: Rarely needed for typical use cases\")\n",
        "print(f\"  target_patterns: {target_patterns} - Optional patterns for selecting specific modules\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING HYPERPARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# num_epochs = 3  # Number of training epochs\n",
        "num_epochs = 1  # Number of training epochs\n",
        "seed = 42  # Random seed for reproducibility\n",
        "lr_scheduler = \"cosine\"  # Learning rate scheduler\n",
        "# lr_scheduler_kwargs = {\"eta_min\": 1e-6}  # Scheduler parameters\n",
        "lr_scheduler_kwargs = {}  # Scheduler parameters\n",
        "# warmup_steps = 100  # Number of warmup steps\n",
        "warmup_steps = 0  # Number of warmup steps\n",
        "\n",
        "print(\"🎯 Training Hyperparameters:\")\n",
        "print(f\"  effective_batch_size: {effective_batch_size} - Effective batch size for training\")\n",
        "print(f\"  learning_rate: {learning_rate} - Learning rate for model updates\")\n",
        "print(f\"  num_epochs: {num_epochs} - Number of training epochs\")\n",
        "print(f\"  lr_scheduler: '{lr_scheduler}' - Learning rate scheduler type\")\n",
        "print(f\"  lr_scheduler_kwargs: {lr_scheduler_kwargs} - Scheduler parameters\")\n",
        "print(f\"  warmup_steps: {warmup_steps} - Number of warmup steps\")\n",
        "print(f\"  seed: {seed} - Random seed for reproducibility\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# MEMORY AND PERFORMANCE PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "use_liger = True  # Use Liger kernels for efficiency\n",
        "\n",
        "print(\"⚡ Memory and Performance Parameters:\")\n",
        "print(f\"  max_tokens_per_gpu: {max_tokens_per_gpu:,} - Maximum tokens per GPU (hard-cap for memory)\")\n",
        "print(f\"  max_seq_len: {max_seq_len:,} - Maximum sequence length\")\n",
        "print(f\"  use_liger: {use_liger} - Use Liger kernels for efficiency\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# DATA PROCESSING PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "data_output_dir = \"/dev/shm/osft_data\"  # Directory for processed data (RAM disk for speed)\n",
        "use_processed_dataset = False  # Whether data is pre-processed\n",
        "unmask_messages = False  # Whether to unmask all messages for pretraining-style learning\n",
        "\n",
        "print(\"💾 Data Processing Parameters:\")\n",
        "print(f\"  data_path: '{data_path}' - Path to training data (JSONL format)\")\n",
        "print(f\"  data_output_dir: '{data_output_dir}' - Directory to save processed data\")\n",
        "print(f\"  use_processed_dataset: {use_processed_dataset} - Whether to use pre-processed data\")\n",
        "print(f\"  unmask_messages: {unmask_messages} - Whether to unmask all messages\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# CHECKPOINTING PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "checkpoint_at_epoch = True  # Whether to checkpoint at each epoch\n",
        "save_final_checkpoint = True  # Whether to save final checkpoint\n",
        "\n",
        "print(\"💾 Checkpointing Parameters:\")\n",
        "print(f\"  ckpt_output_dir: '{ckpt_output_dir}' - Directory to save checkpoints\")\n",
        "print(f\"  checkpoint_at_epoch: {checkpoint_at_epoch} - Whether to checkpoint at each epoch\")\n",
        "print(f\"  save_final_checkpoint: {save_final_checkpoint} - Whether to save final checkpoint\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distributed Training Configuration\n",
        "\n",
        "Configure distributed training for both single-node and multi-node setups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🖥️  Distributed Training Parameters:\n",
            "  Configuration: Single node with 8 GPUs\n",
            "  nproc_per_node: 8 - Number of processes (GPUs) per node\n",
            "  nnodes: 1 - Total number of nodes\n",
            "  node_rank: 0 - Rank of this node (0 to nnodes-1)\n",
            "  rdzv_id: 100 - Unique job ID for rendezvous\n",
            "  rdzv_endpoint: '127.0.0.1:29500' - Master node endpoint for multi-node training\n",
            "\n",
            "📊 Resource Calculation:\n",
            "  Total GPUs: 8 (8 × 1)\n",
            "  Effective batch size: 128\n",
            "  Approximate per-GPU batch size: 16\n",
            "  (Actual micro-batch size determined automatically by gradient accumulation)\n",
            "\n",
            "📝 OSFT Multi-Node Considerations:\n",
            "  • OSFT works seamlessly across multiple nodes\n",
            "  • No special replay buffer coordination needed (unlike SFT)\n",
            "  • Each node processes its data portion with the same unfreeze_rank_ratio\n",
            "  • Gradients are synchronized automatically across all nodes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DISTRIBUTED TRAINING PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration options for different setups\n",
        "distributed_configs = {\n",
        "    \"single_gpu_dev\": {\n",
        "        \"nproc_per_node\": 1,\n",
        "        \"nnodes\": 1,\n",
        "        \"node_rank\": 0,\n",
        "        \"rdzv_id\": 1,\n",
        "        \"rdzv_endpoint\": \"127.0.0.1:29500\",\n",
        "        \"description\": \"Development setup with single GPU\"\n",
        "    },\n",
        "    \"single_node_8gpu\": {\n",
        "        \"nproc_per_node\": 8,\n",
        "        \"nnodes\": 1,\n",
        "        \"node_rank\": 0,\n",
        "        \"rdzv_id\": 100,\n",
        "        \"rdzv_endpoint\": \"127.0.0.1:29500\",\n",
        "        \"description\": \"Single node with 8 GPUs\"\n",
        "    },\n",
        "    \"multi_node_master\": {\n",
        "        \"nproc_per_node\": 8,\n",
        "        \"nnodes\": 2,  # 2 nodes\n",
        "        \"node_rank\": 0,\n",
        "        \"rdzv_id\": 42,\n",
        "        # master node IP\n",
        "        \"rdzv_endpoint\": \"10.241.128.23:1738\",  # Replace with actual master IP\n",
        "        \"description\": \"Multi-node master (rank 0) - 4 nodes total\"\n",
        "    },\n",
        "    \"multi_node_worker\": {\n",
        "        \"nproc_per_node\": 8,\n",
        "        \"nnodes\": 2,  # 2 nodes\n",
        "        \"node_rank\": 1,  # Change this for each worker node (1, 2, 3, ...)\n",
        "        \"rdzv_id\": 42,\n",
        "        \"rdzv_endpoint\": \"10.241.128.23:1738\",  # Same as master\n",
        "        \"description\": \"Multi-node worker (rank 1) - change rank for each worker\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Select your distributed configuration\n",
        "selected_distributed = \"single_node_8gpu\"  # Change this to match your setup\n",
        "dist_config = distributed_configs[selected_distributed]\n",
        "\n",
        "# Extract distributed training parameters\n",
        "nproc_per_node = dist_config[\"nproc_per_node\"]  # Number of processes (GPUs) per node\n",
        "nnodes = dist_config[\"nnodes\"]  # Total number of nodes\n",
        "node_rank = dist_config[\"node_rank\"]  # Rank of this node (0 to nnodes-1)\n",
        "rdzv_id = dist_config[\"rdzv_id\"]  # Unique job ID for rendezvous\n",
        "rdzv_endpoint = dist_config[\"rdzv_endpoint\"]  # Master node endpoint for multi-node training\n",
        "\n",
        "# Calculate total resources\n",
        "total_gpus = nproc_per_node * nnodes\n",
        "per_gpu_batch_size = effective_batch_size // total_gpus\n",
        "\n",
        "print(\"🖥️  Distributed Training Parameters:\")\n",
        "print(f\"  Configuration: {dist_config['description']}\")\n",
        "print(f\"  nproc_per_node: {nproc_per_node} - Number of processes (GPUs) per node\")\n",
        "print(f\"  nnodes: {nnodes} - Total number of nodes\")\n",
        "print(f\"  node_rank: {node_rank} - Rank of this node (0 to nnodes-1)\")\n",
        "print(f\"  rdzv_id: {rdzv_id} - Unique job ID for rendezvous\")\n",
        "print(f\"  rdzv_endpoint: '{rdzv_endpoint}' - Master node endpoint for multi-node training\")\n",
        "print()\n",
        "print(f\"📊 Resource Calculation:\")\n",
        "print(f\"  Total GPUs: {total_gpus} ({nproc_per_node} × {nnodes})\")\n",
        "print(f\"  Effective batch size: {effective_batch_size}\")\n",
        "print(f\"  Approximate per-GPU batch size: {per_gpu_batch_size}\")\n",
        "print(f\"  (Actual micro-batch size determined automatically by gradient accumulation)\")\n",
        "print()\n",
        "\n",
        "# Multi-node setup instructions\n",
        "if nnodes > 1:\n",
        "    print(\"🔧 Multi-Node Setup Instructions:\")\n",
        "    print(f\"  1. Ensure all nodes can reach the master at {rdzv_endpoint}\")\n",
        "    print(f\"  2. Use the same rdzv_id ({rdzv_id}) on all nodes\")\n",
        "    print(f\"  3. Set node_rank to 0 for master, 1,2,3... for workers\")\n",
        "    print(f\"  4. Start training on ALL nodes simultaneously\")\n",
        "    print()\n",
        "\n",
        "# OSFT-specific multi-node considerations\n",
        "print(\"📝 OSFT Multi-Node Considerations:\")\n",
        "print(\"  • OSFT works seamlessly across multiple nodes\")\n",
        "print(\"  • No special replay buffer coordination needed (unlike SFT)\")\n",
        "print(\"  • Each node processes its data portion with the same unfreeze_rank_ratio\")\n",
        "print(\"  • Gradients are synchronized automatically across all nodes\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute Training\n",
        "\n",
        "Now let's run the actual OSFT training with all our configured parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting OSFT Training\n",
            "============================================================\n",
            "Experiment: osft_comprehensive_example_20250829_184010\n",
            "Model: Qwen 2.5 7B Instruct\n",
            "Total GPUs: 8 (8 per node × 1 nodes)\n",
            "Configuration: Single node with 8 GPUs\n",
            "Unfreeze Rank Ratio: 0.25\n",
            "\n",
            "✨ OSFT Advantages:\n",
            "  • No catastrophic forgetting\n",
            "  • No replay buffer needed\n",
            "  • Preserves original model capabilities\n",
            "\n",
            "📋 Final Training Configuration:\n",
            "  model_path: Qwen/Qwen2.5-7B-Instruct\n",
            "  data_path: /mnt/nvme2n1/datasets/tablegpt/train/train_ListExtraction.jsonl\n",
            "  ckpt_output_dir: /mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010\n",
            "  unfreeze_rank_ratio: 0.25\n",
            "  effective_batch_size: 128\n",
            "  max_tokens_per_gpu: 10000\n",
            "  max_seq_len: 8196\n",
            "  learning_rate: 5e-06\n",
            "  num_epochs: 1\n",
            "  data_output_dir: /dev/shm/osft_data\n",
            "  use_processed_dataset: False\n",
            "  unmask_messages: False\n",
            "  warmup_steps: 0\n",
            "  use_liger: True\n",
            "  seed: 42\n",
            "  lr_scheduler: cosine\n",
            "  lr_scheduler_kwargs: {}\n",
            "  checkpoint_at_epoch: True\n",
            "  save_final_checkpoint: True\n",
            "  nproc_per_node: 8\n",
            "  nnodes: 1\n",
            "  node_rank: 0\n",
            "  rdzv_id: 100\n",
            "  rdzv_endpoint: 127.0.0.1:29500\n",
            "\n",
            "============================================================\n",
            "⏳ Training starting...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 537 examples [00:00, 71507.44 examples/s]\n",
            "Ensuring dataset is compatible with legacy format. (num_proc=8): 100%|██████████| 537/537 [00:00<00:00, 2448.62 examples/s]\n",
            "Converting samples into input_ids and labels... (num_proc=8): 100%|██████████| 537/537 [00:01<00:00, 434.03 examples/s]\n",
            "Filter (num_proc=8): 100%|██████████| 537/537 [00:00<00:00, 2260.72 examples/s]\n",
            "Filtering out pretraining samples (num_proc=8): 100%|██████████| 537/537 [00:00<00:00, 2254.73 examples/s]\n",
            "Filtering out pretraining samples (num_proc=8): 100%|██████████| 537/537 [00:00<00:00, 2144.75 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35mOriginal Input: <|im_start|>system\n",
            "You are a helpful assistant that specializes in tables.<|im_end|>\n",
            "<|im_start|>user\n",
            "Objective: Please change the list into a tabulated format with multiple columns. Provide the table using plain text, using vertical bars (|) to separate columns and a new line for each row. Return the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n",
            "\n",
            "## Input:\n",
            "Unnamed: 0 Name Political Party Term start Term end\n",
            "6 Richard Adams, Jr. No party 1786 February 21, 1788\n",
            "19 John Barrett nan 1798 1799\n",
            "37 Francis Wicker (acting) nan May 5, 1818 1819\n",
            "4 John Harvie No party 1785 1786\n",
            "5 William Pennock No party December 10, 1786 1786\n",
            "\n",
            "## Output:\n",
            "{\"table\": \"|Unnamed: 0|Name|Political Party|Term start|Term end|\\n|---|---|---|---|---|\\n|6|Richard Adams, Jr.|No party|1786|February 21, 1788|\\n|19|John Barrett|nan|1798|1799|\\n|37|Francis Wicker (acting)|nan|May 5, 1818|1819|\\n|4|John Harvie|No party|1785|1786|\\n|5|William Pennock|No party|December 10, 1786|1786|\\n\"}\n",
            "\n",
            "## Input:\n",
            "Contestant W L\n",
            "Gina Gershon 1 0\n",
            "Gabourey Sidibe 0 1\n",
            "Serena Williams 1 0\n",
            "Candice Bergen 1 0\n",
            "Naomi Watts 1 0\n",
            "\n",
            "Return the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n",
            "## Output:\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "{\"table\": \"|Contestant|W|L|\\n|---|---|---|\\n|Gina Gershon|1|0|\\n|Gabourey Sidibe|0|1|\\n|Serena Williams|1|0|\\n|Candice Bergen|1|0|\\n|Naomi Watts|1|0|\\n\"}<|im_end|>\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mInstruction ex sample 1: <|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|>{\"table\": \"|Contestant|W|L|\\n|---|---|---|\\n|Gina Gershon|1|0|\\n|Gabourey Sidibe|0|1|\\n|Serena Williams|1|0|\\n|Candice Bergen|1|0|\\n|Naomi Watts|1|0|\\n\"}<|im_end|><|MASK|>\u001b[0m\n",
            "\u001b[35mOriginal Input: <|im_start|>system\n",
            "You are a helpful assistant that specializes in tables.<|im_end|>\n",
            "<|im_start|>user\n",
            "Task: I request you to transform the list below into a table comprising multiple columns. The table should be presented in plain text format, using vertical bars (|) as column dividers and new lines for each row. Return the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n",
            "\n",
            "[Q]:\n",
            "Missile Bits Value Pixel 1 Pixel 0\n",
            "Missile 1 $00 0 0\n",
            "Missile 2 $00 0 0\n",
            "nan $30 1 1\n",
            "nan $C0 1 1\n",
            "Missile 0 $00 0 0\n",
            "\n",
            "[A]:\n",
            "{\"table\": \"|Missile|Bits Value|Pixel 1|Pixel 0|\\n|---|---|---|---|\\n|Missile 1|$00|0|0|\\n|Missile 2|$00|0|0|\\n|nan|$30|1|1|\\n|nan|$C0|1|1|\\n|Missile 0|$00|0|0|\\n\"}\n",
            "\n",
            "[Q]:\n",
            "Detect move Full duplex transfer Propagate deletes Delta copying (block level transfer) LAN Sync Freeware version Portable Encrypts files Detect conflict File lock Open file support Revision control Detect rename\n",
            "No Yes Yes nan Yes Limited Yes Yes Yes nan Yes Yes No\n",
            "No ? ? ? No Yes No nan Yes ? ? No Yes\n",
            "Yes Yes Yes nan Yes Yes nan nan Yes nan nan Yes Yes\n",
            "Yes Yes Yes Yes No No Yes Yes Yes Yes Yes Yes Yes\n",
            "Yes Yes Yes Yes Yes No No nan No, handled by versioning Yes Yes Yes Yes\n",
            "\n",
            "[A]:\n",
            "{\"table\": \"|Detect move|Full duplex transfer|Propagate deletes|Delta copying (block level transfer)|LAN Sync|Freeware version|Portable|Encrypts files|Detect conflict|File lock|Open file support|Revision control|Detect rename|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|No|Yes|Yes|nan|Yes|Limited|Yes|Yes|Yes|nan|Yes|Yes|No|\\n|No|?|?|?|No|Yes|No|nan|Yes|?|?|No|Yes|\\n|Yes|Yes|Yes|nan|Yes|Yes|nan|nan|Yes|nan|nan|Yes|Yes|\\n|Yes|Yes|Yes|Yes|No|No|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\\n|Yes|Yes|Yes|Yes|Yes|No|No|nan|No, handled by versioning|Yes|Yes|Yes|Yes|\\n\"}\n",
            "\n",
            "[Q]:\n",
            "Rank Country Score Electoral process and pluralism Functioning of government Political participation Political culture Civil liberties Category\n",
            "50 Philippines 6.94 9.17 5.71 7.22 4.38 8.24 Flawed democracy\n",
            "9 Finland 9.03 10.0 8.93 7.78 8.75 9.71 Full democracy\n",
            "35 Belgium 7.77 9.58 8.57 5.0 6.88 8.82 Flawed democracy\n",
            "39 South Africa 7.41 7.92 7.86 8.33 5.0 7.94 Flawed democracy\n",
            "47 Bulgaria 7.01 9.17 6.07 7.22 4.38 8.24 Flawed democracy\n",
            "\n",
            "Return the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n",
            "[A]:\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "{\"table\": \"|Rank|Country|Score|Electoral process and pluralism|Functioning of government|Political participation|Political culture|Civil liberties|Category|\\n|---|---|---|---|---|---|---|---|---|\\n|50|Philippines|6.94|9.17|5.71|7.22|4.38|8.24|Flawed democracy|\\n|9|Finland|9.03|10.0|8.93|7.78|8.75|9.71|Full democracy|\\n|35|Belgium|7.77|9.58|8.57|5.0|6.88|8.82|Flawed democracy|\\n|39|South Africa|7.41|7.92|7.86|8.33|5.0|7.94|Flawed democracy|\\n|47|Bulgaria|7.01|9.17|6.07|7.22|4.38|8.24|Flawed democracy|\\n\"}<|im_end|>\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mInstruction ex sample 2: <|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|>{\"table\": \"|Rank|Country|Score|Electoral process and pluralism|Functioning of government|Political participation|Political culture|Civil liberties|Category|\\n|---|---|---|---|---|---|---|---|---|\\n|50|Philippines|6.94|9.17|5.71|7.22|4.38|8.24|Flawed democracy|\\n|9|Finland|9.03|10.0|8.93|7.78|8.75|9.71|Full democracy|\\n|35|Belgium|7.77|9.58|8.57|5.0|6.88|8.82|Flawed democracy|\\n|39|South Africa|7.41|7.92|7.86|8.33|5.0|7.94|Flawed democracy|\\n|47|Bulgaria|7.01|9.17|6.07|7.22|4.38|8.24|Flawed democracy|\\n\"}<|im_end|><|MASK|>\u001b[0m\n",
            "\u001b[35mOriginal Input: <|im_start|>system\n",
            "You are a helpful assistant that specializes in tables.<|im_end|>\n",
            "<|im_start|>user\n",
            "Description: I would like you to change the list into a table that contains multiple columns. Use plain text for the table representation, with vertical bars (|) as column separators and a new line for each row. Return the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n",
            "\n",
            "## Input:\n",
            "No. Title Directed by Written by Release Date Prod. code\n",
            "2 \"Aliens & Sex & Chips & Gravy\" Scott Handcock James Goss August 2017 BFPTWFIVE01CD\n",
            "10 TBA Scott Handcock TBA February 2018 BFPTWFIVE03CD\n",
            "8 TBA Scott Handcock TBA October 2017 BFPTWFIVE02CD\n",
            "11 TBA Scott Handcock TBA February 2018 BFPTWFIVE03CD\n",
            "12 TBA Scott Handcock TBA February 2018 BFPTWFIVE03CD\n",
            "\n",
            "Return the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n",
            "## Output:\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "{\"table\": \"|No.|Title|Directed by|Written by|Release Date|Prod. code|\\n|---|---|---|---|---|---|\\n|2|\\\"Aliens & Sex & Chips & Gravy\\\"|Scott Handcock|James Goss|August 2017|BFPTWFIVE01CD|\\n|10|TBA|Scott Handcock|TBA|February 2018|BFPTWFIVE03CD|\\n|8|TBA|Scott Handcock|TBA|October 2017|BFPTWFIVE02CD|\\n|11|TBA|Scott Handcock|TBA|February 2018|BFPTWFIVE03CD|\\n|12|TBA|Scott Handcock|TBA|February 2018|BFPTWFIVE03CD|\\n\"}<|im_end|>\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mInstruction ex sample 3: <|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|><|MASK|>{\"table\": \"|No.|Title|Directed by|Written by|Release Date|Prod. code|\\n|---|---|---|---|---|---|\\n|2|\\\"Aliens & Sex & Chips & Gravy\\\"|Scott Handcock|James Goss|August 2017|BFPTWFIVE01CD|\\n|10|TBA|Scott Handcock|TBA|February 2018|BFPTWFIVE03CD|\\n|8|TBA|Scott Handcock|TBA|October 2017|BFPTWFIVE02CD|\\n|11|TBA|Scott Handcock|TBA|February 2018|BFPTWFIVE03CD|\\n|12|TBA|Scott Handcock|TBA|February 2018|BFPTWFIVE03CD|\\n\"}<|im_end|><|MASK|>\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating unmask tokens not in data (num_proc=8): 100%|██████████| 537/537 [00:00<00:00, 2056.57 examples/s]\n",
            "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 11.44ba/s]\n",
            "2025-08-29 18:40:30,526 - mini_trainer.api_train - INFO - Starting training setup...\n",
            "2025-08-29 18:40:30,527 - mini_trainer.api_train - INFO - Running training command as subprocess: torchrun --nnodes=1 --node_rank=0 --nproc_per_node=8 --rdzv_id=100 --rdzv_endpoint=127.0.0.1:29500 /home/vpcuser/osilkin/mini_trainer/src/mini_trainer/train.py --model-name-or-path=Qwen/Qwen2.5-7B-Instruct --data-path=/dev/shm/osft_data/data.jsonl --batch-size=128 --max-tokens-per-gpu=10000 --learning-rate=5e-06 --num-warmup-steps=0 --lr-scheduler=cosine --lr-scheduler-kwargs={} --seed=42 --output-dir=/mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010 --training-mode=epoch --max-epochs=1 --max-steps=0 --max-tokens=0 --use-liger-kernels --osft --osft-unfreeze-rank-ratio=0.25 --checkpoint-at-epoch --save-final-checkpoint\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command: torchrun --nnodes=1 --node_rank=0 --nproc_per_node=8 --rdzv_id=100 --rdzv_endpoint=127.0.0.1:29500 /home/vpcuser/osilkin/mini_trainer/src/mini_trainer/train.py --model-name-or-path=Qwen/Qwen2.5-7B-Instruct --data-path=/dev/shm/osft_data/data.jsonl --batch-size=128 --max-tokens-per-gpu=10000 --learning-rate=5e-06 --num-warmup-steps=0 --lr-scheduler=cosine --lr-scheduler-kwargs={} --seed=42 --output-dir=/mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010 --training-mode=epoch --max-epochs=1 --max-steps=0 --max-tokens=0 --use-liger-kernels --osft --osft-unfreeze-rank-ratio=0.25 --checkpoint-at-epoch --save-final-checkpoint\n",
            "Logs will be saved to: /mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010/training_log_node0.log\n",
            "W0829 18:40:31.670000 2797721 torch/distributed/run.py:774] \n",
            "W0829 18:40:31.670000 2797721 torch/distributed/run.py:774] *****************************************\n",
            "W0829 18:40:31.670000 2797721 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0829 18:40:31.670000 2797721 torch/distributed/run.py:774] *****************************************\n",
            "✅ Torch distributed appears to be functioning correctly\n",
            "Training with parameters: {\n",
            "    \"model_name_or_path\":\"Qwen/Qwen2.5-7B-Instruct\",\n",
            "    \"data_path\":\"/dev/shm/osft_data/data.jsonl\",\n",
            "    \"batch_size\":128,\n",
            "    \"max_tokens_per_gpu\":10000,\n",
            "    \"learning_rate\":5e-06,\n",
            "    \"num_warmup_steps\":0,\n",
            "    \"lr_scheduler\":\"cosine\",\n",
            "    \"seed\":42,\n",
            "    \"use_liger_kernels\":true,\n",
            "    \"osft\":true,\n",
            "    \"osft_unfreeze_rank_ratio\":0.25,\n",
            "    \"osft_target_patterns\":null,\n",
            "    \"osft_upcast_dtype\":\"float32\",\n",
            "    \"osft_output_dtype\":null,\n",
            "    \"output_dir\":\"/mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010\",\n",
            "    \"min_samples_per_checkpoint\":null,\n",
            "    \"save_dtype\":null,\n",
            "    \"training_mode\":\"epoch\",\n",
            "    \"max_epochs\":1,\n",
            "    \"max_steps\":0,\n",
            "    \"max_tokens\":0,\n",
            "    \"checkpoint_at_epoch\":true,\n",
            "    \"save_final_checkpoint\":true,\n",
            "    \"RANK\":0,\n",
            "    \"WORLD_SIZE\":8\n",
            "}\n",
            "Training parameters saved to /mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010/training_params.json\n",
            "\n",
            "Generating train split: 0 examples [00:00, ? examples/s]\n",
            "Generating train split: 537 examples [00:00, 7521.49 examples/s]\n",
            "EPOCH training mode: 5 batches/epoch * 1 epochs = 5 steps\n",
            "Calculated num_training_steps: 5\n",
            "\u001b[2;36m[18:40:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=77597;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=890580;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Applied Liger kernels to Qwen2\n",
            "\u001b[2;36m[18:40:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=845917;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=680069;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Applied Liger kernels to Qwen2\n",
            "\u001b[2;36m[18:40:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=288119;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=348222;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Applied Liger kernels to Qwen2\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\u001b[2;36m[18:40:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=872828;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=11034;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Applied Liger kernels to Qwen2\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[2;36m[18:40:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=636758;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=485774;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Applied Liger kernels to Qwen2\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\u001b[2;36m[18:40:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=133192;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=974245;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Applied Liger kernels to Qwen2\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[2;36m[18:40:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=92289;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=715112;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Applied Liger kernels to Qwen2\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[2;36m[18:40:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Applying Liger kernels for model type:  \u001b]8;id=24596;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py\u001b\\\u001b[2mmonkey_patch.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=966108;file:///home/vpcuser/osilkin/training_hub/.venv/lib/python3.12/site-packages/liger_kernel/transformers/monkey_patch.py#1991\u001b\\\u001b[2m1991\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m         qwen2 with kwargs: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[2m                    \u001b[0m\n",
            "Applied Liger kernels to Qwen2\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.23s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.02s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.30s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.27s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.36s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:18<00:54, 18.31s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.37s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.37s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.41s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.44s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.46s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.45s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.47s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:38, 19.04s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.47s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.39s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.49s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.51s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.49s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.46s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.51s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.51s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:57<00:19, 19.27s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:58<00:19, 19.51s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 18.89s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.06s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 18.91s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.10s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.78s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.86s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 18.92s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 18.92s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.11s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.11s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 18.92s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.12s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 18.93s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.12s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 18.92s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.12s/it]\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[33m!!!! Calling from_pretrained !!!!\u001b[0m\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLMWithOSFT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.01s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  2.00s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.00s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.11s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.14s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.16s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.14s/it]\n",
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.17s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.99s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.99s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.97s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.10s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.12s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.13s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.13s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.14s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.11s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.12s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.15s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.28s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.38s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.38s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.39s/it]\n",
            "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.44s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.46s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.31s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.53s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]\n",
            "\u001b[33m!!!! loading osft config !!!!\u001b[0m\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.60s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.59s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.60s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.46s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.57s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]\n",
            "🚀 Computing distributed OSFT decomposition across all ranks\n",
            "Distributing OSFT work across 8 ranks\n",
            "\n",
            "[OSFT Init Rank 5] Decomposing params:   0%|          | 0/24 [00:00<?, ?it/s]\n",
            "[OSFT Init Rank 3] Decomposing params:   0%|          | 0/25 [00:00<?, ?it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:   0%|          | 0/24 [00:00<?, ?it/s]\n",
            "[OSFT Init Rank 5] Decomposing params:   4%|▍         | 1/24 [00:01<00:39,  1.72s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:   4%|▍         | 1/25 [00:01<00:41,  1.72s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:   8%|▊         | 2/24 [00:03<00:34,  1.59s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:   4%|▍         | 1/24 [00:01<00:41,  1.78s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  12%|█▎        | 3/24 [00:01<00:10,  1.97it/s]\n",
            "[OSFT Init Rank 3] Decomposing params:   8%|▊         | 2/25 [00:03<00:37,  1.62s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  12%|█▎        | 3/24 [00:04<00:32,  1.54s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  17%|█▋        | 4/24 [00:03<00:17,  1.17it/s]\n",
            "[OSFT Init Rank 3] Decomposing params:  12%|█▏        | 3/25 [00:04<00:34,  1.57s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[33m!!!! Initializing OSFT Params!!!!\u001b[0m\n",
            "\n",
            "[OSFT Init Rank 1] Decomposing params:   0%|          | 0/25 [00:00<?, ?it/s]\n",
            "[OSFT Init Rank 4] Decomposing params:   0%|          | 0/24 [00:00<?, ?it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:   0%|          | 0/24 [00:00<?, ?it/s]\n",
            "[OSFT Init Rank 5] Decomposing params:  25%|██▌       | 6/24 [00:06<00:15,  1.18it/s]\n",
            "[OSFT Init Rank 2] Decomposing params:   0%|          | 0/25 [00:00<?, ?it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:  21%|██        | 5/24 [00:04<00:20,  1.09s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  16%|█▌        | 4/25 [00:06<00:32,  1.55s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:   4%|▍         | 1/25 [00:00<00:10,  2.29it/s]\n",
            "[OSFT Init Rank 2] Decomposing params:   4%|▍         | 1/25 [00:00<00:08,  2.78it/s]\n",
            "[OSFT Init Rank 0] Decomposing params:   4%|▍         | 1/25 [00:01<00:41,  1.72s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  12%|█▏        | 3/25 [00:01<00:10,  2.02it/s]\n",
            "[OSFT Init Rank 5] Decomposing params:  29%|██▉       | 7/24 [00:07<00:17,  1.01s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  20%|██        | 5/25 [00:07<00:30,  1.52s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  25%|██▌       | 6/24 [00:06<00:22,  1.23s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  12%|█▏        | 3/25 [00:01<00:14,  1.49it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:   4%|▍         | 1/24 [00:01<00:44,  1.95s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:   8%|▊         | 2/25 [00:01<00:23,  1.01s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:   4%|▍         | 1/24 [00:01<00:45,  2.00s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  16%|█▌        | 4/25 [00:03<00:17,  1.23it/s]\n",
            "[OSFT Init Rank 5] Decomposing params:  33%|███▎      | 8/24 [00:09<00:18,  1.13s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  32%|███▏      | 8/25 [00:09<00:15,  1.12it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:  29%|██▉       | 7/24 [00:08<00:22,  1.31s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  16%|█▌        | 4/25 [00:03<00:20,  1.04it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:   8%|▊         | 2/24 [00:03<00:36,  1.68s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  12%|█▏        | 3/25 [00:03<00:27,  1.24s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:   8%|▊         | 2/24 [00:03<00:37,  1.70s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  20%|██        | 5/25 [00:04<00:21,  1.06s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  38%|███▊      | 9/24 [00:10<00:18,  1.23s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  33%|███▎      | 8/24 [00:09<00:21,  1.35s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  36%|███▌      | 9/25 [00:10<00:16,  1.05s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  20%|██        | 5/25 [00:04<00:22,  1.14s/it]\n",
            "[OSFT Init Rank 6] Decomposing params:  21%|██        | 5/24 [00:04<00:15,  1.21it/s]\n",
            "[OSFT Init Rank 4] Decomposing params:  12%|█▎        | 3/24 [00:04<00:33,  1.60s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  16%|█▌        | 4/25 [00:04<00:28,  1.34s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  24%|██▍       | 6/25 [00:06<00:23,  1.22s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  42%|████▏     | 10/24 [00:12<00:17,  1.28s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  46%|████▌     | 11/24 [00:10<00:11,  1.15it/s]\n",
            "[OSFT Init Rank 3] Decomposing params:  40%|████      | 10/25 [00:12<00:17,  1.16s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  24%|██▍       | 6/25 [00:06<00:23,  1.25s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  17%|█▋        | 4/24 [00:06<00:30,  1.54s/it]\n",
            "[OSFT Init Rank 6] Decomposing params:  25%|██▌       | 6/24 [00:06<00:17,  1.00it/s]\n",
            "[OSFT Init Rank 2] Decomposing params:  20%|██        | 5/25 [00:06<00:27,  1.39s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  28%|██▊       | 7/25 [00:07<00:23,  1.32s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  54%|█████▍    | 13/24 [00:13<00:09,  1.17it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:  50%|█████     | 12/24 [00:12<00:12,  1.01s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  44%|████▍     | 11/25 [00:13<00:17,  1.25s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  28%|██▊       | 7/25 [00:07<00:23,  1.31s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  24%|██▍       | 6/25 [00:07<00:26,  1.40s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  29%|██▉       | 7/24 [00:07<00:14,  1.14it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:  29%|██▉       | 7/24 [00:07<00:19,  1.12s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  32%|███▏      | 8/25 [00:09<00:23,  1.36s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  58%|█████▊    | 14/24 [00:15<00:09,  1.01it/s]\n",
            "[OSFT Init Rank 3] Decomposing params:  48%|████▊     | 12/25 [00:15<00:16,  1.29s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  40%|████      | 10/25 [00:09<00:12,  1.19it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:  54%|█████▍    | 13/24 [00:14<00:12,  1.14s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  36%|███▌      | 9/25 [00:09<00:13,  1.16it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:  33%|███▎      | 8/24 [00:09<00:19,  1.22s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  33%|███▎      | 8/24 [00:09<00:16,  1.02s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  44%|████▍     | 11/25 [00:10<00:12,  1.15it/s]\n",
            "[OSFT Init Rank 5] Decomposing params:  62%|██████▎   | 15/24 [00:16<00:09,  1.10s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  60%|██████    | 15/25 [00:16<00:08,  1.16it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:  38%|███▊      | 9/24 [00:10<00:19,  1.28s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  58%|█████▊    | 14/24 [00:15<00:12,  1.23s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  40%|████      | 10/25 [00:10<00:15,  1.00s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  44%|████▍     | 11/25 [00:10<00:13,  1.01it/s]\n",
            "[OSFT Init Rank 4] Decomposing params:  38%|███▊      | 9/24 [00:10<00:17,  1.14s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  48%|████▊     | 12/25 [00:12<00:13,  1.01s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  67%|██████▋   | 16/24 [00:18<00:09,  1.20s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  64%|██████▍   | 16/25 [00:18<00:08,  1.01it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:  50%|█████     | 12/24 [00:12<00:10,  1.17it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:  62%|██████▎   | 15/24 [00:17<00:11,  1.29s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  44%|████▍     | 11/25 [00:12<00:15,  1.12s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  42%|████▏     | 10/24 [00:12<00:17,  1.23s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  48%|████▊     | 12/25 [00:12<00:14,  1.11s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  52%|█████▏    | 13/25 [00:13<00:13,  1.12s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  71%|███████   | 17/24 [00:19<00:08,  1.25s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  68%|██████▊   | 17/25 [00:19<00:08,  1.10s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  46%|████▌     | 11/24 [00:13<00:16,  1.29s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  75%|███████▌  | 18/24 [00:18<00:05,  1.15it/s]\n",
            "[OSFT Init Rank 2] Decomposing params:  48%|████▊     | 12/25 [00:13<00:15,  1.22s/it]\n",
            "[OSFT Init Rank 6] Decomposing params:  54%|█████▍    | 13/24 [00:13<00:10,  1.00it/s]\n",
            "[OSFT Init Rank 1] Decomposing params:  52%|█████▏    | 13/25 [00:13<00:14,  1.20s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  83%|████████▎ | 20/24 [00:20<00:03,  1.17it/s]\n",
            "[OSFT Init Rank 0] Decomposing params:  56%|█████▌    | 14/25 [00:15<00:13,  1.21s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  72%|███████▏  | 18/25 [00:21<00:08,  1.19s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  52%|█████▏    | 13/25 [00:15<00:15,  1.28s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  56%|█████▌    | 14/25 [00:15<00:13,  1.27s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  58%|█████▊    | 14/24 [00:15<00:08,  1.15it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:  79%|███████▉  | 19/24 [00:20<00:05,  1.00s/it]\n",
            "[OSFT Init Rank 6] Decomposing params:  58%|█████▊    | 14/24 [00:15<00:11,  1.11s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  60%|██████    | 15/25 [00:16<00:12,  1.27s/it]\n",
            "[OSFT Init Rank 5] Decomposing params:  88%|████████▊ | 21/24 [00:22<00:02,  1.02it/s]\n",
            "[OSFT Init Rank 3] Decomposing params:  76%|███████▌  | 19/25 [00:22<00:07,  1.25s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  64%|██████▍   | 16/25 [00:16<00:07,  1.15it/s]\n",
            "[OSFT Init Rank 1] Decomposing params:  68%|██████▊   | 17/25 [00:16<00:06,  1.16it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:  62%|██████▎   | 15/24 [00:16<00:10,  1.20s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  62%|██████▎   | 15/24 [00:16<00:09,  1.00s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  83%|████████▎ | 20/24 [00:21<00:04,  1.12s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  72%|███████▏  | 18/25 [00:18<00:06,  1.16it/s]\n",
            "[OSFT Init Rank 5] Decomposing params:  92%|█████████▏| 22/24 [00:23<00:02,  1.09s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  88%|████████▊ | 22/25 [00:24<00:02,  1.17it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:  67%|██████▋   | 16/24 [00:18<00:10,  1.26s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  68%|██████▊   | 17/25 [00:18<00:08,  1.00s/it]\n",
            "[OSFT Init Rank 7] Decomposing params:  88%|████████▊ | 21/24 [00:23<00:03,  1.21s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  72%|███████▏  | 18/25 [00:18<00:07,  1.00s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  67%|██████▋   | 16/24 [00:18<00:08,  1.12s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  76%|███████▌  | 19/25 [00:19<00:05,  1.00it/s]\n",
            "[OSFT Init Rank 5] Decomposing params:  96%|█████████▌| 23/24 [00:25<00:01,  1.19s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  92%|█████████▏| 23/25 [00:25<00:01,  1.02it/s]\n",
            "[OSFT Init Rank 6] Decomposing params:  79%|███████▉  | 19/24 [00:19<00:04,  1.17it/s]\n",
            "[OSFT Init Rank 7] Decomposing params:  92%|█████████▏| 22/24 [00:24<00:02,  1.27s/it]\n",
            "[OSFT Init Rank 7] Decomposing params: 100%|██████████| 24/24 [00:24<00:00,  1.02s/it]\n",
            "\n",
            "[OSFT Init Rank 2] Decomposing params:  72%|███████▏  | 18/25 [00:19<00:07,  1.12s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  76%|███████▌  | 19/25 [00:19<00:06,  1.11s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  71%|███████   | 17/24 [00:19<00:08,  1.21s/it]\n",
            "[OSFT Init Rank 5] Decomposing params: 100%|██████████| 24/24 [00:26<00:00,  1.25s/it]\n",
            "[OSFT Init Rank 5] Decomposing params: 100%|██████████| 24/24 [00:26<00:00,  1.12s/it]\n",
            "\n",
            "[OSFT Init Rank 0] Decomposing params:  80%|████████  | 20/25 [00:21<00:05,  1.11s/it]\n",
            "[OSFT Init Rank 3] Decomposing params:  96%|█████████▌| 24/25 [00:27<00:01,  1.10s/it]\n",
            "[OSFT Init Rank 6] Decomposing params:  83%|████████▎ | 20/24 [00:21<00:03,  1.02it/s]\n",
            "[OSFT Init Rank 4] Decomposing params:  75%|███████▌  | 18/24 [00:21<00:07,  1.27s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  76%|███████▌  | 19/25 [00:21<00:07,  1.21s/it]\n",
            "[OSFT Init Rank 1] Decomposing params:  80%|████████  | 20/25 [00:21<00:06,  1.21s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  84%|████████▍ | 21/25 [00:22<00:04,  1.20s/it]\n",
            "[OSFT Init Rank 3] Decomposing params: 100%|██████████| 25/25 [00:28<00:00,  1.19s/it]\n",
            "[OSFT Init Rank 3] Decomposing params: 100%|██████████| 25/25 [00:28<00:00,  1.14s/it]\n",
            "\n",
            "[OSFT Init Rank 6] Decomposing params:  88%|████████▊ | 21/24 [00:22<00:03,  1.09s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  88%|████████▊ | 21/24 [00:22<00:02,  1.16it/s]\n",
            "[OSFT Init Rank 1] Decomposing params:  84%|████████▍ | 21/25 [00:22<00:05,  1.28s/it]\n",
            "[OSFT Init Rank 2] Decomposing params:  80%|████████  | 20/25 [00:22<00:06,  1.29s/it]\n",
            "[OSFT Init Rank 0] Decomposing params:  88%|████████▊ | 22/25 [00:24<00:03,  1.26s/it]\n",
            "[OSFT Init Rank 6] Decomposing params:  92%|█████████▏| 22/24 [00:24<00:02,  1.19s/it]\n",
            "[OSFT Init Rank 4] Decomposing params:  92%|█████████▏| 22/24 [00:24<00:01,  1.00it/s]\n",
            "[OSFT Init Rank 1] Decomposing params:  96%|█████████▌| 24/25 [00:24<00:00,  1.15it/s]\n",
            "[OSFT Init Rank 2] Decomposing params:  92%|█████████▏| 23/25 [00:24<00:01,  1.15it/s]\n",
            "[OSFT Init Rank 0] Decomposing params: 100%|██████████| 25/25 [00:25<00:00,  1.16it/s]\n",
            "[OSFT Init Rank 0] Decomposing params: 100%|██████████| 25/25 [00:25<00:00,  1.03s/it]\n",
            "\n",
            "[OSFT Init Rank 6] Decomposing params:  96%|█████████▌| 23/24 [00:25<00:01,  1.25s/it]\n",
            "[OSFT Init Rank 6] Decomposing params: 100%|██████████| 24/24 [00:25<00:00,  1.07s/it]\n",
            "\n",
            "[OSFT Init Rank 2] Decomposing params:  96%|█████████▌| 24/25 [00:25<00:00,  1.01it/s]\n",
            "[OSFT Init Rank 1] Decomposing params: 100%|██████████| 25/25 [00:25<00:00,  1.01it/s]\n",
            "[OSFT Init Rank 1] Decomposing params: 100%|██████████| 25/25 [00:25<00:00,  1.03s/it]\n",
            "\n",
            "[OSFT Init Rank 4] Decomposing params:  96%|█████████▌| 23/24 [00:25<00:01,  1.11s/it]\n",
            "[OSFT Init Rank 4] Decomposing params: 100%|██████████| 24/24 [00:27<00:00,  1.20s/it]\n",
            "[OSFT Init Rank 4] Decomposing params: 100%|██████████| 24/24 [00:27<00:00,  1.14s/it]\n",
            "\n",
            "[OSFT Init Rank 2] Decomposing params: 100%|██████████| 25/25 [00:27<00:00,  1.10s/it]\n",
            "[OSFT Init Rank 2] Decomposing params: 100%|██████████| 25/25 [00:27<00:00,  1.09s/it]\n",
            "✅ Distributed OSFT decomposition complete\n",
            "\u001b[38;2;255;255;0mWarning: Model class name: Qwen2ForCausalLMWithOSFT is not in the list of supported models.\u001b[0m\n",
            "Using FSDP2 wrapper\n",
            "\u001b[92m{\n",
            "    \"step\": 1,\n",
            "    \"lr\": 3.272542485937369e-06,\n",
            "    \"grad_norm\": 2.286625862121582,\n",
            "    \"loss\": 0.10476521233633553,\n",
            "    \"avg_loss_backward\": 0.05238259583711624,\n",
            "    \"num_samples\": 128.0,\n",
            "    \"num_loss_counted_tokens\": 21748.0,\n",
            "    \"batch_num_loss_counted_tokens\": 21748,\n",
            "    \"num_total_tokens\": 100916.0,\n",
            "    \"grad_accum\": 2,\n",
            "    \"avg_time_per_minibatch\": 21.466777801513672,\n",
            "    \"time_per_batch\": 44.57166767120361,\n",
            "    \"tokens_per_second\": 2264.1288799072404,\n",
            "    \"total_samples_accumulated\": 128.0,\n",
            "    \"total_tokens_accumulated\": 21748,\n",
            "    \"samples_per_second\": 2.871779466369325,\n",
            "    \"peak_memory_usage_GB\": 47.743857152,\n",
            "    \"timestamp\": \"2025-08-29T18:43:50.446351\"\n",
            "}\u001b[0m\n",
            "\u001b[92m{\n",
            "    \"step\": 2,\n",
            "    \"lr\": 1.7274575140626318e-06,\n",
            "    \"grad_norm\": 0.3664357364177704,\n",
            "    \"loss\": 0.01944632941792191,\n",
            "    \"avg_loss_backward\": 0.00972316600382328,\n",
            "    \"num_samples\": 128.0,\n",
            "    \"num_loss_counted_tokens\": 20182.0,\n",
            "    \"batch_num_loss_counted_tokens\": 20182,\n",
            "    \"num_total_tokens\": 103740.0,\n",
            "    \"grad_accum\": 2,\n",
            "    \"avg_time_per_minibatch\": 21.20982551574707,\n",
            "    \"time_per_batch\": 42.62193012237549,\n",
            "    \"tokens_per_second\": 2433.9582863127775,\n",
            "    \"total_samples_accumulated\": 256.0,\n",
            "    \"total_tokens_accumulated\": 41930,\n",
            "    \"samples_per_second\": 3.0031488398692456,\n",
            "    \"peak_memory_usage_GB\": 50.943627776,\n",
            "    \"timestamp\": \"2025-08-29T18:44:33.249384\"\n",
            "}\u001b[0m\n",
            "\u001b[92m{\n",
            "    \"step\": 3,\n",
            "    \"lr\": 4.774575140626317e-07,\n",
            "    \"grad_norm\": 0.31723806262016296,\n",
            "    \"loss\": 0.02163306077930669,\n",
            "    \"avg_loss_backward\": 0.01081653032451868,\n",
            "    \"num_samples\": 128.0,\n",
            "    \"num_loss_counted_tokens\": 23478.0,\n",
            "    \"batch_num_loss_counted_tokens\": 23478,\n",
            "    \"num_total_tokens\": 99843.0,\n",
            "    \"grad_accum\": 2,\n",
            "    \"avg_time_per_minibatch\": 21.072845458984375,\n",
            "    \"time_per_batch\": 42.44777870178223,\n",
            "    \"tokens_per_second\": 2352.1372155054123,\n",
            "    \"total_samples_accumulated\": 384.0,\n",
            "    \"total_tokens_accumulated\": 65408,\n",
            "    \"samples_per_second\": 3.0154699236270224,\n",
            "    \"peak_memory_usage_GB\": 50.950940672,\n",
            "    \"timestamp\": \"2025-08-29T18:45:16.053370\"\n",
            "}\u001b[0m\n",
            "\u001b[92m{\n",
            "    \"step\": 4,\n",
            "    \"lr\": 0.0,\n",
            "    \"grad_norm\": 0.172865092754364,\n",
            "    \"loss\": 0.012564289404301307,\n",
            "    \"avg_loss_backward\": 0.006282145623117685,\n",
            "    \"num_samples\": 128.0,\n",
            "    \"num_loss_counted_tokens\": 22034.0,\n",
            "    \"batch_num_loss_counted_tokens\": 22034,\n",
            "    \"num_total_tokens\": 103397.0,\n",
            "    \"grad_accum\": 2,\n",
            "    \"avg_time_per_minibatch\": 21.149688720703125,\n",
            "    \"time_per_batch\": 42.43429970741272,\n",
            "    \"tokens_per_second\": 2436.6373597049815,\n",
            "    \"total_samples_accumulated\": 512.0,\n",
            "    \"total_tokens_accumulated\": 87442,\n",
            "    \"samples_per_second\": 3.0164277691058508,\n",
            "    \"peak_memory_usage_GB\": 50.94220032,\n",
            "    \"timestamp\": \"2025-08-29T18:45:58.694744\"\n",
            "}\u001b[0m\n",
            "\u001b[92m{\n",
            "    \"step\": 5,\n",
            "    \"lr\": 4.774575140626311e-07,\n",
            "    \"grad_norm\": 0.4162523150444031,\n",
            "    \"loss\": 0.018424443492949166,\n",
            "    \"avg_loss_backward\": 0.018424443900585175,\n",
            "    \"num_samples\": 25.0,\n",
            "    \"num_loss_counted_tokens\": 4149.0,\n",
            "    \"batch_num_loss_counted_tokens\": 4149,\n",
            "    \"num_total_tokens\": 20607.0,\n",
            "    \"grad_accum\": 1,\n",
            "    \"avg_time_per_minibatch\": 10.087742805480957,\n",
            "    \"time_per_batch\": 10.347558736801147,\n",
            "    \"tokens_per_second\": 1991.4842258117458,\n",
            "    \"total_samples_accumulated\": 537.0,\n",
            "    \"total_tokens_accumulated\": 91591,\n",
            "    \"samples_per_second\": 2.416028807943594,\n",
            "    \"peak_memory_usage_GB\": 49.359469056,\n",
            "    \"timestamp\": \"2025-08-29T18:46:09.118294\"\n",
            "}\u001b[0m\n",
            "Saving model at 537.0 samples\n",
            "Reconstructing OSFT weights for checkpoint saving...\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   1%|          | 1/196 [00:00<01:37,  1.99it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   2%|▏         | 4/196 [00:01<00:45,  4.23it/s]Processed 5/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   3%|▎         | 5/196 [00:03<02:52,  1.11it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   3%|▎         | 6/196 [00:06<04:25,  1.40s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   4%|▎         | 7/196 [00:08<05:28,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   4%|▍         | 8/196 [00:09<04:18,  1.38s/it]Processed 10/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   6%|▌         | 11/196 [00:09<02:11,  1.41it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   6%|▌         | 12/196 [00:12<03:24,  1.11s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   7%|▋         | 13/196 [00:15<04:27,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   7%|▋         | 14/196 [00:17<05:16,  1.74s/it]Processed 15/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   8%|▊         | 15/196 [00:18<04:16,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   9%|▉         | 18/196 [00:18<02:15,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  10%|▉         | 19/196 [00:21<03:21,  1.14s/it]Processed 20/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  10%|█         | 20/196 [00:24<04:18,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  11%|█         | 21/196 [00:26<05:04,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  11%|█         | 22/196 [00:27<04:07,  1.42s/it]Processed 25/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  13%|█▎        | 25/196 [00:27<02:11,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  13%|█▎        | 26/196 [00:30<03:13,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  14%|█▍        | 27/196 [00:32<04:07,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  14%|█▍        | 28/196 [00:35<04:51,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  15%|█▍        | 29/196 [00:35<03:57,  1.42s/it]Processed 30/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  16%|█▋        | 32/196 [00:36<02:05,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  17%|█▋        | 33/196 [00:39<03:05,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  17%|█▋        | 34/196 [00:41<03:57,  1.46s/it]Processed 35/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  18%|█▊        | 35/196 [00:44<04:39,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  18%|█▊        | 36/196 [00:44<03:46,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  20%|█▉        | 39/196 [00:45<02:00,  1.31it/s]Processed 40/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  20%|██        | 40/196 [00:47<02:57,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  21%|██        | 41/196 [00:50<03:46,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  21%|██▏       | 42/196 [00:53<04:26,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  22%|██▏       | 43/196 [00:53<03:36,  1.41s/it]Processed 45/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  23%|██▎       | 46/196 [00:54<01:54,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  24%|██▍       | 47/196 [00:56<02:49,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  24%|██▍       | 48/196 [00:59<03:36,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  25%|██▌       | 49/196 [01:01<04:14,  1.73s/it]Processed 50/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  26%|██▌       | 50/196 [01:02<03:26,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  27%|██▋       | 53/196 [01:02<01:49,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  28%|██▊       | 54/196 [01:05<02:41,  1.14s/it]Processed 55/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  28%|██▊       | 55/196 [01:08<03:26,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  29%|██▊       | 56/196 [01:10<04:02,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  29%|██▉       | 57/196 [01:11<03:16,  1.42s/it]Processed 60/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  31%|███       | 60/196 [01:11<01:44,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  31%|███       | 61/196 [01:14<02:33,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  32%|███▏      | 62/196 [01:17<03:16,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  32%|███▏      | 63/196 [01:19<03:50,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  33%|███▎      | 64/196 [01:20<03:06,  1.41s/it]Processed 65/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  34%|███▍      | 67/196 [01:20<01:38,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  35%|███▍      | 68/196 [01:23<02:25,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  35%|███▌      | 69/196 [01:25<03:05,  1.46s/it]Processed 70/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  36%|███▌      | 70/196 [01:28<03:38,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  36%|███▌      | 71/196 [01:28<02:56,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  38%|███▊      | 74/196 [01:29<01:33,  1.30it/s]Processed 75/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  38%|███▊      | 75/196 [01:32<02:17,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  39%|███▉      | 76/196 [01:34<02:55,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  39%|███▉      | 77/196 [01:37<03:26,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  40%|███▉      | 78/196 [01:37<02:47,  1.42s/it]Processed 80/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  41%|████▏     | 81/196 [01:38<01:28,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  42%|████▏     | 82/196 [01:40<02:09,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  42%|████▏     | 83/196 [01:43<02:45,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  43%|████▎     | 84/196 [01:46<03:14,  1.73s/it]Processed 85/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  43%|████▎     | 85/196 [01:46<02:37,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  45%|████▍     | 88/196 [01:47<01:22,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  45%|████▌     | 89/196 [01:49<02:01,  1.14s/it]Processed 90/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  46%|████▌     | 90/196 [01:52<02:35,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  46%|████▋     | 91/196 [01:54<03:01,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  47%|████▋     | 92/196 [01:55<02:27,  1.42s/it]Processed 95/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  48%|████▊     | 95/196 [01:55<01:17,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  49%|████▉     | 96/196 [01:58<01:53,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  49%|████▉     | 97/196 [02:01<02:24,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  50%|█████     | 98/196 [02:03<02:49,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  51%|█████     | 99/196 [02:04<02:17,  1.41s/it]Processed 100/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  52%|█████▏    | 102/196 [02:04<01:11,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  53%|█████▎    | 103/196 [02:07<01:45,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  53%|█████▎    | 104/196 [02:10<02:14,  1.46s/it]Processed 105/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  54%|█████▎    | 105/196 [02:12<02:37,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  54%|█████▍    | 106/196 [02:13<02:07,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  56%|█████▌    | 109/196 [02:13<01:06,  1.31it/s]Processed 110/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  56%|█████▌    | 110/196 [02:16<01:37,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  57%|█████▋    | 111/196 [02:18<02:04,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  57%|█████▋    | 112/196 [02:21<02:25,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  58%|█████▊    | 113/196 [02:21<01:57,  1.42s/it]Processed 115/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  59%|█████▉    | 116/196 [02:22<01:01,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  60%|█████▉    | 117/196 [02:25<01:29,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  60%|██████    | 118/196 [02:27<01:54,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  61%|██████    | 119/196 [02:30<02:13,  1.74s/it]Processed 120/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  61%|██████    | 120/196 [02:30<01:47,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  63%|██████▎   | 123/196 [02:31<00:56,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  63%|██████▎   | 124/196 [02:33<01:21,  1.14s/it]Processed 125/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  64%|██████▍   | 125/196 [02:36<01:44,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  64%|██████▍   | 126/196 [02:39<02:01,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  65%|██████▍   | 127/196 [02:39<01:37,  1.41s/it]Processed 130/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  66%|██████▋   | 130/196 [02:40<00:50,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  67%|██████▋   | 131/196 [02:42<01:13,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  67%|██████▋   | 132/196 [02:45<01:33,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  68%|██████▊   | 133/196 [02:47<01:49,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  68%|██████▊   | 134/196 [02:48<01:27,  1.42s/it]Processed 135/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  70%|██████▉   | 137/196 [02:48<00:45,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  70%|███████   | 138/196 [02:51<01:05,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  71%|███████   | 139/196 [02:54<01:23,  1.46s/it]Processed 140/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  71%|███████▏  | 140/196 [02:56<01:36,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  72%|███████▏  | 141/196 [02:57<01:17,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  73%|███████▎  | 144/196 [02:57<00:39,  1.31it/s]Processed 145/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  74%|███████▍  | 145/196 [03:00<00:57,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  74%|███████▍  | 146/196 [03:02<01:13,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  75%|███████▌  | 147/196 [03:05<01:24,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  76%|███████▌  | 148/196 [03:06<01:07,  1.41s/it]Processed 150/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  77%|███████▋  | 151/196 [03:06<00:34,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  78%|███████▊  | 152/196 [03:09<00:49,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  78%|███████▊  | 153/196 [03:11<01:02,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  79%|███████▊  | 154/196 [03:14<01:12,  1.73s/it]Processed 155/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  79%|███████▉  | 155/196 [03:14<00:57,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  81%|████████  | 158/196 [03:15<00:29,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  81%|████████  | 159/196 [03:18<00:41,  1.13s/it]Processed 160/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  82%|████████▏ | 160/196 [03:20<00:52,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  82%|████████▏ | 161/196 [03:23<01:00,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  83%|████████▎ | 162/196 [03:23<00:48,  1.41s/it]Processed 165/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  84%|████████▍ | 165/196 [03:24<00:23,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  85%|████████▍ | 166/196 [03:26<00:33,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  85%|████████▌ | 167/196 [03:29<00:42,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  86%|████████▌ | 168/196 [03:32<00:48,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  86%|████████▌ | 169/196 [03:32<00:38,  1.41s/it]Processed 170/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  88%|████████▊ | 172/196 [03:33<00:18,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  88%|████████▊ | 173/196 [03:35<00:26,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  89%|████████▉ | 174/196 [03:38<00:32,  1.46s/it]Processed 175/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  89%|████████▉ | 175/196 [03:40<00:36,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  90%|████████▉ | 176/196 [03:41<00:28,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  91%|█████████▏| 179/196 [03:41<00:13,  1.30it/s]Processed 180/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  92%|█████████▏| 180/196 [03:44<00:18,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  92%|█████████▏| 181/196 [03:47<00:22,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  93%|█████████▎| 182/196 [03:49<00:24,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  93%|█████████▎| 183/196 [03:50<00:18,  1.42s/it]Processed 185/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  95%|█████████▍| 186/196 [03:50<00:07,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  95%|█████████▌| 187/196 [03:53<00:10,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  96%|█████████▌| 188/196 [03:56<00:11,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  96%|█████████▋| 189/196 [03:58<00:12,  1.74s/it]Processed 190/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  97%|█████████▋| 190/196 [03:59<00:08,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  98%|█████████▊| 193/196 [03:59<00:02,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  99%|█████████▉| 194/196 [04:02<00:02,  1.14s/it]Processed 195/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  99%|█████████▉| 195/196 [04:04<00:01,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...: 100%|██████████| 196/196 [04:07<00:00,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...: 100%|██████████| 196/196 [04:07<00:00,  1.26s/it]\n",
            "Finished reconstructing 196 OSFT parameters\n",
            "⚠️  Warning: Found tensor model.embed_tokens.weight with dtype torch.float32, casting to torch.bfloat16\n",
            "✅ Saved model at 537.0 samples in 271.79 seconds\n",
            "\n",
            "Saving model at 537.0 samples\n",
            "Reconstructing OSFT weights for checkpoint saving...\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   1%|          | 1/196 [00:00<01:36,  2.02it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   2%|▏         | 4/196 [00:01<00:44,  4.27it/s]Processed 5/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   3%|▎         | 5/196 [00:03<02:51,  1.11it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   3%|▎         | 6/196 [00:06<04:23,  1.39s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   4%|▎         | 7/196 [00:08<05:27,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   4%|▍         | 8/196 [00:09<04:17,  1.37s/it]Processed 10/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   6%|▌         | 11/196 [00:09<02:10,  1.42it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:   6%|▌         | 12/196 [00:12<03:22,  1.10s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   7%|▋         | 13/196 [00:15<04:25,  1.45s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   7%|▋         | 14/196 [00:17<05:13,  1.72s/it]Processed 15/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:   8%|▊         | 15/196 [00:18<04:13,  1.40s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:   9%|▉         | 18/196 [00:18<02:13,  1.33it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  10%|▉         | 19/196 [00:21<03:18,  1.12s/it]Processed 20/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  10%|█         | 20/196 [00:23<04:16,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  11%|█         | 21/196 [00:26<05:02,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  11%|█         | 22/196 [00:26<04:05,  1.41s/it]Processed 25/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  13%|█▎        | 25/196 [00:27<02:09,  1.32it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  13%|█▎        | 26/196 [00:30<03:12,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  14%|█▍        | 27/196 [00:32<04:06,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  14%|█▍        | 28/196 [00:35<04:50,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  15%|█▍        | 29/196 [00:35<03:56,  1.41s/it]Processed 30/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  16%|█▋        | 32/196 [00:36<02:05,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  17%|█▋        | 33/196 [00:38<03:04,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  17%|█▋        | 34/196 [00:41<03:56,  1.46s/it]Processed 35/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  18%|█▊        | 35/196 [00:44<04:38,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  18%|█▊        | 36/196 [00:44<03:45,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  20%|█▉        | 39/196 [00:45<01:59,  1.31it/s]Processed 40/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  20%|██        | 40/196 [00:47<02:56,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  21%|██        | 41/196 [00:50<03:46,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  21%|██▏       | 42/196 [00:52<04:26,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  22%|██▏       | 43/196 [00:53<03:36,  1.41s/it]Processed 45/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  23%|██▎       | 46/196 [00:53<01:54,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  24%|██▍       | 47/196 [00:56<02:48,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  24%|██▍       | 48/196 [00:59<03:36,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  25%|██▌       | 49/196 [01:01<04:14,  1.73s/it]Processed 50/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  26%|██▌       | 50/196 [01:02<03:26,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  27%|██▋       | 53/196 [01:02<01:49,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  28%|██▊       | 54/196 [01:05<02:40,  1.13s/it]Processed 55/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  28%|██▊       | 55/196 [01:07<03:26,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  29%|██▊       | 56/196 [01:10<04:02,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  29%|██▉       | 57/196 [01:10<03:16,  1.41s/it]Processed 60/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  31%|███       | 60/196 [01:11<01:43,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  31%|███       | 61/196 [01:14<02:32,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  32%|███▏      | 62/196 [01:16<03:15,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  32%|███▏      | 63/196 [01:19<03:50,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  33%|███▎      | 64/196 [01:19<03:06,  1.41s/it]Processed 65/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  34%|███▍      | 67/196 [01:20<01:38,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  35%|███▍      | 68/196 [01:22<02:25,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  35%|███▌      | 69/196 [01:25<03:05,  1.46s/it]Processed 70/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  36%|███▌      | 70/196 [01:28<03:38,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  36%|███▌      | 71/196 [01:28<02:56,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  38%|███▊      | 74/196 [01:29<01:33,  1.31it/s]Processed 75/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  38%|███▊      | 75/196 [01:31<02:17,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  39%|███▉      | 76/196 [01:34<02:55,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  39%|███▉      | 77/196 [01:36<03:26,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  40%|███▉      | 78/196 [01:37<02:46,  1.41s/it]Processed 80/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  41%|████▏     | 81/196 [01:37<01:27,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  42%|████▏     | 82/196 [01:40<02:09,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  42%|████▏     | 83/196 [01:43<02:45,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  43%|████▎     | 84/196 [01:45<03:13,  1.73s/it]Processed 85/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  43%|████▎     | 85/196 [01:46<02:36,  1.41s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  45%|████▍     | 88/196 [01:46<01:22,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  45%|████▌     | 89/196 [01:49<02:01,  1.13s/it]Processed 90/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  46%|████▌     | 90/196 [01:51<02:35,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  46%|████▋     | 91/196 [01:54<03:01,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  47%|████▋     | 92/196 [01:55<02:27,  1.41s/it]Processed 95/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  48%|████▊     | 95/196 [01:55<01:17,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  49%|████▉     | 96/196 [01:58<01:53,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  49%|████▉     | 97/196 [02:00<02:24,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  50%|█████     | 98/196 [02:03<02:49,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  51%|█████     | 99/196 [02:03<02:17,  1.41s/it]Processed 100/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  52%|█████▏    | 102/196 [02:04<01:11,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  53%|█████▎    | 103/196 [02:07<01:45,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  53%|█████▎    | 104/196 [02:09<02:14,  1.47s/it]Processed 105/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  54%|█████▎    | 105/196 [02:12<02:37,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  54%|█████▍    | 106/196 [02:12<02:07,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  56%|█████▌    | 109/196 [02:13<01:06,  1.31it/s]Processed 110/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  56%|█████▌    | 110/196 [02:15<01:37,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  57%|█████▋    | 111/196 [02:18<02:04,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  57%|█████▋    | 112/196 [02:21<02:25,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  58%|█████▊    | 113/196 [02:21<01:57,  1.42s/it]Processed 115/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  59%|█████▉    | 116/196 [02:22<01:01,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  60%|█████▉    | 117/196 [02:24<01:29,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  60%|██████    | 118/196 [02:27<01:54,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  61%|██████    | 119/196 [02:29<02:13,  1.73s/it]Processed 120/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  61%|██████    | 120/196 [02:30<01:47,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  63%|██████▎   | 123/196 [02:30<00:55,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  63%|██████▎   | 124/196 [02:33<01:21,  1.14s/it]Processed 125/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  64%|██████▍   | 125/196 [02:36<01:44,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  64%|██████▍   | 126/196 [02:38<02:01,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  65%|██████▍   | 127/196 [02:39<01:37,  1.42s/it]Processed 130/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  66%|██████▋   | 130/196 [02:39<00:50,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  67%|██████▋   | 131/196 [02:42<01:13,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  67%|██████▋   | 132/196 [02:44<01:33,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  68%|██████▊   | 133/196 [02:47<01:49,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  68%|██████▊   | 134/196 [02:48<01:28,  1.42s/it]Processed 135/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  70%|██████▉   | 137/196 [02:48<00:45,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  70%|███████   | 138/196 [02:51<01:06,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  71%|███████   | 139/196 [02:53<01:23,  1.47s/it]Processed 140/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  71%|███████▏  | 140/196 [02:56<01:37,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  72%|███████▏  | 141/196 [02:56<01:17,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  73%|███████▎  | 144/196 [02:57<00:39,  1.31it/s]Processed 145/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  74%|███████▍  | 145/196 [03:00<00:57,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  74%|███████▍  | 146/196 [03:02<01:13,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  75%|███████▌  | 147/196 [03:05<01:24,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  76%|███████▌  | 148/196 [03:05<01:07,  1.41s/it]Processed 150/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  77%|███████▋  | 151/196 [03:06<00:34,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  78%|███████▊  | 152/196 [03:08<00:49,  1.13s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  78%|███████▊  | 153/196 [03:11<01:02,  1.46s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  79%|███████▊  | 154/196 [03:14<01:12,  1.73s/it]Processed 155/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  79%|███████▉  | 155/196 [03:14<00:58,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  81%|████████  | 158/196 [03:15<00:29,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  81%|████████  | 159/196 [03:17<00:42,  1.14s/it]Processed 160/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  82%|████████▏ | 160/196 [03:20<00:52,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  82%|████████▏ | 161/196 [03:22<01:00,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  83%|████████▎ | 162/196 [03:23<00:48,  1.42s/it]Processed 165/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  84%|████████▍ | 165/196 [03:23<00:23,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  85%|████████▍ | 166/196 [03:26<00:34,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  85%|████████▌ | 167/196 [03:29<00:42,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  86%|████████▌ | 168/196 [03:31<00:48,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  86%|████████▌ | 169/196 [03:32<00:38,  1.42s/it]Processed 170/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  88%|████████▊ | 172/196 [03:32<00:18,  1.30it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  88%|████████▊ | 173/196 [03:35<00:26,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  89%|████████▉ | 174/196 [03:38<00:32,  1.47s/it]Processed 175/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  89%|████████▉ | 175/196 [03:40<00:36,  1.74s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  90%|████████▉ | 176/196 [03:41<00:28,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  91%|█████████▏| 179/196 [03:41<00:13,  1.30it/s]Processed 180/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  92%|█████████▏| 180/196 [03:44<00:18,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  92%|█████████▏| 181/196 [03:46<00:21,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  93%|█████████▎| 182/196 [03:49<00:24,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  93%|█████████▎| 183/196 [03:50<00:18,  1.42s/it]Processed 185/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  95%|█████████▍| 186/196 [03:50<00:07,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  95%|█████████▌| 187/196 [03:53<00:10,  1.14s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  96%|█████████▌| 188/196 [03:55<00:11,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  96%|█████████▋| 189/196 [03:58<00:12,  1.73s/it]Processed 190/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  97%|█████████▋| 190/196 [03:58<00:08,  1.42s/it]\n",
            "Reconstructing OSFT weights, this may take a while...:  98%|█████████▊| 193/196 [03:59<00:02,  1.31it/s]\n",
            "Reconstructing OSFT weights, this may take a while...:  99%|█████████▉| 194/196 [04:02<00:02,  1.14s/it]Processed 195/196 OSFT parameters\n",
            "\n",
            "Reconstructing OSFT weights, this may take a while...:  99%|█████████▉| 195/196 [04:04<00:01,  1.47s/it]\n",
            "Reconstructing OSFT weights, this may take a while...: 100%|██████████| 196/196 [04:07<00:00,  1.73s/it]\n",
            "Reconstructing OSFT weights, this may take a while...: 100%|██████████| 196/196 [04:07<00:00,  1.26s/it]\n",
            "Finished reconstructing 196 OSFT parameters\n",
            "⚠️  Warning: Found tensor model.embed_tokens.weight with dtype torch.float32, casting to torch.bfloat16\n",
            "✅ Saved model at 537.0 samples in 276.96 seconds\n",
            "\n",
            "[rank0]:[W829 18:55:18.919700218 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-29 18:55:21,707 - mini_trainer.api_train - INFO - Training completed successfully! 🎉\n",
            "2025-08-29 18:55:21,708 - mini_trainer.api_train - INFO - Waiting for process to exit (60s timeout)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "✅ OSFT Training completed successfully!\n",
            "⏱️  Total duration: 0.25 hours (15.0 minutes)\n",
            "📁 Checkpoints saved to: /mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010\n",
            "============================================================\n",
            "\n",
            "🎯 What you've achieved with OSFT:\n",
            "  • Model adapted to new domain/task\n",
            "  • Original capabilities preserved\n",
            "  • No catastrophic forgetting occurred\n",
            "  • Ready for deployment without regression testing!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TRAINING EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"🚀 Starting OSFT Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Experiment: {full_experiment_name}\")\n",
        "print(f\"Model: {selected_example['model_name']}\")\n",
        "print(f\"Total GPUs: {total_gpus} ({nproc_per_node} per node × {nnodes} nodes)\")\n",
        "print(f\"Configuration: {dist_config['description']}\")\n",
        "print(f\"Unfreeze Rank Ratio: {unfreeze_rank_ratio}\")\n",
        "print()\n",
        "print(\"✨ OSFT Advantages:\")\n",
        "print(\"  • No catastrophic forgetting\")\n",
        "print(\"  • No replay buffer needed\")\n",
        "print(\"  • Preserves original model capabilities\")\n",
        "print()\n",
        "\n",
        "# Prepare all training parameters\n",
        "training_params = {\n",
        "    # Required parameters\n",
        "    'model_path': model_path,\n",
        "    'data_path': data_path,\n",
        "    'ckpt_output_dir': ckpt_output_dir,\n",
        "    'unfreeze_rank_ratio': unfreeze_rank_ratio,\n",
        "    'effective_batch_size': effective_batch_size,\n",
        "    'max_tokens_per_gpu': max_tokens_per_gpu,\n",
        "    'max_seq_len': max_seq_len,\n",
        "    'learning_rate': learning_rate,\n",
        "    \n",
        "    # Optional OSFT-specific parameters\n",
        "    'target_patterns': target_patterns,\n",
        "    \n",
        "    # Training duration\n",
        "    'num_epochs': num_epochs,\n",
        "    \n",
        "    # Data processing parameters\n",
        "    'data_output_dir': data_output_dir,\n",
        "    'use_processed_dataset': use_processed_dataset,\n",
        "    'unmask_messages': unmask_messages,\n",
        "    'warmup_steps': warmup_steps,\n",
        "    \n",
        "    # Optimization parameters\n",
        "    'use_liger': use_liger,\n",
        "    'seed': seed,\n",
        "    'lr_scheduler': lr_scheduler,\n",
        "    'lr_scheduler_kwargs': lr_scheduler_kwargs,\n",
        "    \n",
        "    # Checkpointing parameters\n",
        "    'checkpoint_at_epoch': checkpoint_at_epoch,\n",
        "    'save_final_checkpoint': save_final_checkpoint,\n",
        "    \n",
        "    # Distributed training parameters\n",
        "    'nproc_per_node': nproc_per_node,\n",
        "    'nnodes': nnodes,\n",
        "    'node_rank': node_rank,\n",
        "    'rdzv_id': rdzv_id,\n",
        "    'rdzv_endpoint': rdzv_endpoint,\n",
        "}\n",
        "\n",
        "# Display final configuration summary\n",
        "print(\"📋 Final Training Configuration:\")\n",
        "for key, value in training_params.items():\n",
        "    if value is not None:  # Only show non-None values\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"⏳ Training starting...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Execute training\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    result = osft(**training_params)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ OSFT Training completed successfully!\")\n",
        "    print(f\"⏱️  Total duration: {duration/3600:.2f} hours ({duration/60:.1f} minutes)\")\n",
        "    print(f\"📁 Checkpoints saved to: {ckpt_output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    print(\"🎯 What you've achieved with OSFT:\")\n",
        "    print(\"  • Model adapted to new domain/task\")\n",
        "    print(\"  • Original capabilities preserved\")\n",
        "    print(\"  • No catastrophic forgetting occurred\")\n",
        "    print(\"  • Ready for deployment without regression testing!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"❌ Training failed after {duration/60:.1f} minutes\")\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(\"\\n🔍 Quick Troubleshooting Checklist:\")\n",
        "    print(\"  □ Check that model_path exists or is a valid HuggingFace model name\")\n",
        "    print(\"  □ Verify data_path points to valid JSONL file\")\n",
        "    print(\"  □ Ensure ckpt_output_dir parent directory exists and is writable\")\n",
        "    print(\"  □ Try reducing max_tokens_per_gpu if you see OOM errors\")\n",
        "    print(\"  □ Try adjusting unfreeze_rank_ratio (lower = more preservation)\")\n",
        "    print(\"  □ For multi-node: verify network connectivity and endpoints\")\n",
        "    print(\"  □ Check that mini-trainer backend dependencies are installed\")\n",
        "    \n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Training Analysis\n",
        "\n",
        "After training completes, let's analyze the results and provide guidance for next steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Post-Training Analysis\n",
            "==================================================\n",
            "✅ Found 1 checkpoint(s):\n",
            "  📁 hf_format\n",
            "\n",
            "🎯 Final model checkpoint: /mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010/hf_format\n",
            "\n",
            "💻 Model Loading Example:\n",
            "```python\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "# Load your OSFT-adapted model\n",
            "model = AutoModelForCausalLM.from_pretrained('/mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010/hf_format')\n",
            "tokenizer = AutoTokenizer.from_pretrained('/mnt/nvme2n1/checkpoints/osft_comprehensive_example_20250829_184010/hf_format')\n",
            "\n",
            "# Test the model - it should maintain original capabilities\n",
            "# while excelling at your new domain/task\n",
            "inputs = tokenizer('Your domain-specific prompt:', return_tensors='pt')\n",
            "outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True)\n",
            "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
            "print(response)\n",
            "```\n",
            "\n",
            "📈 Training Summary:\n",
            "  Model: Qwen 2.5 7B Instruct\n",
            "  Algorithm: OSFT (Orthogonal Subspace Fine-Tuning)\n",
            "  Unfreeze Rank Ratio: 0.25\n",
            "  Epochs: 1\n",
            "  Global Batch Size: 128\n",
            "  Learning Rate: 5e-06\n",
            "  Max Tokens per GPU: 10,000\n",
            "  Max Sequence Length: 8,196\n",
            "  Total GPUs: 8\n",
            "  Distributed Config: Single node with 8 GPUs\n",
            "\n",
            "🧪 OSFT-Specific Validation Steps:\n",
            "  1. **Test Original Capabilities**: Verify the model still performs well on\n",
            "     general tasks it was originally trained for\n",
            "  2. **Test New Domain**: Confirm improved performance on your target domain\n",
            "  3. **No Regression Testing Needed**: Unlike SFT, OSFT preserves capabilities\n",
            "     by design, reducing validation overhead\n",
            "  4. **Compare with Base Model**: Run side-by-side comparisons to see\n",
            "     improvements without degradation\n",
            "\n",
            "🚀 Recommended Next Steps:\n",
            "  1. 🎯 Test on domain-specific evaluation sets\n",
            "  2. 📊 Compare performance with base model on both general and domain tasks\n",
            "  3. 🔄 If more adaptation needed, slightly increase unfreeze_rank_ratio\n",
            "  4. 💡 If too much change occurred, reduce unfreeze_rank_ratio\n",
            "  5. 📝 Document the unfreeze_rank_ratio that works best for your use case\n",
            "  6. 🚢 Deploy with confidence - no catastrophic forgetting!\n",
            "\n",
            "⚡ OSFT-Specific Optimization Tips:\n",
            "  • Current unfreeze_rank_ratio (0.25):\n",
            "    Balanced - good preservation with reasonable adaptation\n",
            "    This is ideal for most use cases\n",
            "  • Memory usage is similar to SFT - adjust max_tokens_per_gpu as needed\n",
            "  • For production: use the script version for better logging and resumption\n",
            "\n",
            "✨ OSFT Training Complete!\n",
            "Your model has been successfully adapted without forgetting!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# POST-TRAINING ANALYSIS AND NEXT STEPS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"📊 Post-Training Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check for saved checkpoints\n",
        "checkpoint_dir = ckpt_output_dir\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = [d for d in os.listdir(checkpoint_dir) \n",
        "                  if os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
        "    \n",
        "    if checkpoints:\n",
        "        print(f\"✅ Found {len(checkpoints)} checkpoint(s):\")\n",
        "        for ckpt in sorted(checkpoints):\n",
        "            ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
        "            print(f\"  📁 {ckpt}\")\n",
        "        \n",
        "        # Identify the final checkpoint\n",
        "        final_checkpoint = sorted(checkpoints)[-1]\n",
        "        final_checkpoint_path = os.path.join(checkpoint_dir, final_checkpoint)\n",
        "        \n",
        "        print(f\"\\n🎯 Final model checkpoint: {final_checkpoint_path}\")\n",
        "        \n",
        "        # Provide model loading example\n",
        "        print(f\"\\n💻 Model Loading Example:\")\n",
        "        print(f\"```python\")\n",
        "        print(f\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
        "        print(f\"\")\n",
        "        print(f\"# Load your OSFT-adapted model\")\n",
        "        print(f\"model = AutoModelForCausalLM.from_pretrained('{final_checkpoint_path}')\")\n",
        "        print(f\"tokenizer = AutoTokenizer.from_pretrained('{final_checkpoint_path}')\")\n",
        "        print(f\"\")\n",
        "        print(f\"# Test the model - it should maintain original capabilities\")\n",
        "        print(f\"# while excelling at your new domain/task\")\n",
        "        print(f\"inputs = tokenizer('Your domain-specific prompt:', return_tensors='pt')\")\n",
        "        print(f\"outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True)\")\n",
        "        print(f\"response = tokenizer.decode(outputs[0], skip_special_tokens=True)\")\n",
        "        print(f\"print(response)\")\n",
        "        print(f\"```\")\n",
        "    else:\n",
        "        print(f\"❌ No checkpoints found in {checkpoint_dir}\")\n",
        "else:\n",
        "    print(f\"❌ Checkpoint directory not found: {checkpoint_dir}\")\n",
        "\n",
        "# Training summary\n",
        "print(f\"\\n📈 Training Summary:\")\n",
        "print(f\"  Model: {selected_example['model_name']}\")\n",
        "print(f\"  Algorithm: OSFT (Orthogonal Subspace Fine-Tuning)\")\n",
        "print(f\"  Unfreeze Rank Ratio: {unfreeze_rank_ratio}\")\n",
        "print(f\"  Epochs: {num_epochs}\")\n",
        "print(f\"  Global Batch Size: {effective_batch_size}\")\n",
        "print(f\"  Learning Rate: {learning_rate}\")\n",
        "print(f\"  Max Tokens per GPU: {max_tokens_per_gpu:,}\")\n",
        "print(f\"  Max Sequence Length: {max_seq_len:,}\")\n",
        "print(f\"  Total GPUs: {total_gpus}\")\n",
        "print(f\"  Distributed Config: {dist_config['description']}\")\n",
        "\n",
        "# OSFT-specific validation recommendations\n",
        "print(f\"\\n🧪 OSFT-Specific Validation Steps:\")\n",
        "print(f\"  1. **Test Original Capabilities**: Verify the model still performs well on\")\n",
        "print(f\"     general tasks it was originally trained for\")\n",
        "print(f\"  2. **Test New Domain**: Confirm improved performance on your target domain\")\n",
        "print(f\"  3. **No Regression Testing Needed**: Unlike SFT, OSFT preserves capabilities\")\n",
        "print(f\"     by design, reducing validation overhead\")\n",
        "print(f\"  4. **Compare with Base Model**: Run side-by-side comparisons to see\")\n",
        "print(f\"     improvements without degradation\")\n",
        "\n",
        "# Next steps recommendations\n",
        "print(f\"\\n🚀 Recommended Next Steps:\")\n",
        "print(f\"  1. 🎯 Test on domain-specific evaluation sets\")\n",
        "print(f\"  2. 📊 Compare performance with base model on both general and domain tasks\")\n",
        "print(f\"  3. 🔄 If more adaptation needed, slightly increase unfreeze_rank_ratio\")\n",
        "print(f\"  4. 💡 If too much change occurred, reduce unfreeze_rank_ratio\")\n",
        "print(f\"  5. 📝 Document the unfreeze_rank_ratio that works best for your use case\")\n",
        "print(f\"  6. 🚢 Deploy with confidence - no catastrophic forgetting!\")\n",
        "\n",
        "# Performance optimization tips\n",
        "print(f\"\\n⚡ OSFT-Specific Optimization Tips:\")\n",
        "print(f\"  • Current unfreeze_rank_ratio ({unfreeze_rank_ratio}):\")\n",
        "if unfreeze_rank_ratio < 0.2:\n",
        "    print(f\"    Very conservative - great preservation, slower adaptation\")\n",
        "    print(f\"    Consider increasing to 0.25-0.3 if need more adaptation\")\n",
        "elif unfreeze_rank_ratio < 0.35:\n",
        "    print(f\"    Balanced - good preservation with reasonable adaptation\")\n",
        "    print(f\"    This is ideal for most use cases\")\n",
        "else:\n",
        "    print(f\"    Aggressive - faster adaptation, slightly less preservation\")\n",
        "    print(f\"    Consider reducing if seeing any capability degradation\")\n",
        "\n",
        "print(f\"  • Memory usage is similar to SFT - adjust max_tokens_per_gpu as needed\")\n",
        "print(f\"  • For production: use the script version for better logging and resumption\")\n",
        "\n",
        "print(f\"\\n✨ OSFT Training Complete!\")\n",
        "print(f\"Your model has been successfully adapted without forgetting!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Reference Summary\n",
        "\n",
        "Quick reference for all OSFT parameters and their purposes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Required Parameters\n",
        "\n",
        "| Parameter | Description | Example Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `model_path` | Path to the model to fine-tune | `\"Qwen/Qwen2.5-7B\"`, `\"/path/to/model\"` |\n",
        "| `data_path` | Path to the training data | `\"/path/to/train.jsonl\"` |\n",
        "| `ckpt_output_dir` | Directory to save checkpoints | `\"/path/to/checkpoints\"` |\n",
        "| `unfreeze_rank_ratio` | **OSFT-specific**: Controls preservation vs adaptation | `0.25`, `0.3`, `0.4` |\n",
        "| `effective_batch_size` | Effective batch size for training | `64`, `128`, `256` |\n",
        "| `max_tokens_per_gpu` | Maximum tokens per GPU (memory limit) | `16384`, `25000`, `40000` |\n",
        "| `max_seq_len` | Maximum sequence length | `2048`, `8192`, `32768` |\n",
        "| `learning_rate` | Learning rate for training | `1e-5`, `2e-5`, `5e-6` |\n",
        "\n",
        "### OSFT-Specific Parameters\n",
        "\n",
        "| Parameter | Description | Recommended Values | Use Case |\n",
        "|-----------|-------------|-------------------|----------|\n",
        "| `unfreeze_rank_ratio` | Controls how much of each matrix is unfrozen | `0.1-0.3` | Conservative preservation |\n",
        "|           |             | `0.3-0.5` | Balanced adaptation |\n",
        "|           |             | `>0.5` | Rarely needed |\n",
        "| `target_patterns` | Optional patterns to match specific modules | `None` | Default (all modules) |\n",
        "\n",
        "### Training Configuration Parameters\n",
        "\n",
        "| Parameter | Description | Default/Example |\n",
        "|-----------|-------------|-----------------|\n",
        "| `num_epochs` | Number of training epochs | `1` |\n",
        "| `seed` | Random seed for reproducibility | `42` |\n",
        "| `use_liger` | Enable Liger kernels for efficiency | `False` |\n",
        "| `warmup_steps` | Number of warmup steps | `0` |\n",
        "| `lr_scheduler` | Learning rate scheduler | `\"cosine\"` |\n",
        "| `lr_scheduler_kwargs` | Additional scheduler parameters | `{\"eta_min\": 1e-6}` |\n",
        "\n",
        "### Data Processing Parameters\n",
        "\n",
        "| Parameter | Description | Default/Example |\n",
        "|-----------|-------------|-----------------|\n",
        "| `data_output_dir` | Directory to save processed data | Defaults to `f\"{ckpt_output_dir}/_internal_data_processing\"`, Recommended value is `\"/dev/shm\"` (shared memory) |\n",
        "| `use_processed_dataset` | Use pre-processed data with input_ids/labels | `False` |\n",
        "| `unmask_messages` | Unmask all messages for pretraining-style learning | `False` |\n",
        "\n",
        "### Checkpointing Parameters\n",
        "\n",
        "| Parameter | Description | Recommended |\n",
        "|-----------|-------------|-------------|\n",
        "| `checkpoint_at_epoch` | Whether to checkpoint at each epoch | `True` |\n",
        "| `save_final_checkpoint` | Whether to save final checkpoint | `True` |\n",
        "\n",
        "### Distributed Training Parameters\n",
        "\n",
        "| Parameter | Description | Example Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `nproc_per_node` | Number of processes (GPUs) per node | `1`, `4`, `8` |\n",
        "| `nnodes` | Total number of nodes | `1`, `2`, `4` |\n",
        "| `node_rank` | Rank of this node (0 to nnodes-1) | `0` (master), `1`, `2`... |\n",
        "| `rdzv_id` | Unique job ID for rendezvous | `42`, `100` |\n",
        "| `rdzv_endpoint` | Master node endpoint for multi-node training | `\"127.0.0.1:29500\"` |\n",
        "\n",
        "### Unfreeze Rank Ratio Guidelines\n",
        "\n",
        "| Use Case | Recommended Ratio | Rationale |\n",
        "|----------|-------------------|-----------|\n",
        "| **Minor format changes** | 0.1-0.15 | Maximum preservation, minimal changes |\n",
        "| **Domain vocabulary addition** | 0.15-0.25 | Add specialized terms without losing general knowledge |\n",
        "| **Domain specialization** | 0.25-0.35 | Balance between preservation and adaptation |\n",
        "| **Major capability expansion** | 0.35-0.5 | More freedom for significant new capabilities |\n",
        "| **Complete repurposing** | >0.5 | Rarely needed, approaching standard fine-tuning |\n",
        "\n",
        "### OSFT vs SFT Key Differences\n",
        "\n",
        "| Aspect | OSFT | SFT |\n",
        "|--------|------|-----|\n",
        "| **Catastrophic Forgetting** | Prevented by design | Requires replay buffers |\n",
        "| **Data Requirements** | Only new domain data | Needs mixed/replay data |\n",
        "| **Memory Usage** | Similar to SFT | Similar to OSFT |\n",
        "| **Key Parameter** | `unfreeze_rank_ratio` | N/A |\n",
        "| **Backend** | mini-trainer | instructlab-training |\n",
        "| **Best For** | Continual learning, domain adaptation | Initial fine-tuning |\n",
        "\n",
        "### Popular Model Examples for OSFT\n",
        "\n",
        "| Model | HuggingFace Path | Recommended `unfreeze_rank_ratio` | `max_tokens_per_gpu` |\n",
        "|-------|------------------|-----------------------------------|----------------------|\n",
        "| Qwen 2.5 7B | `Qwen/Qwen2.5-7B-Instruct` | 0.25 | 10000 |\n",
        "| Llama 3.1 8B | `meta-llama/Meta-Llama-3.1-8B-Instruct` | 0.3 | 10000 |\n",
        "| Phi 4 Mini | `microsoft/Phi-4-mini-instruct` | 0.25 | 15000 |\n",
        "\n",
        "### Script Alternative\n",
        "\n",
        "For production workloads or long-running training, use the script version:\n",
        "\n",
        "```bash\n",
        "# Qwen example\n",
        "python scripts/osft_qwen_example.py \\\n",
        "  --data-path /path/to/data.jsonl \\\n",
        "  --ckpt-output-dir /path/to/checkpoints \\\n",
        "  --unfreeze-rank-ratio 0.25\n",
        "\n",
        "# Llama example\n",
        "python scripts/osft_llama_example.py \\\n",
        "  --data-path /path/to/data.jsonl \\\n",
        "  --ckpt-output-dir /path/to/checkpoints \\\n",
        "  --unfreeze-rank-ratio 0.3\n",
        "\n",
        "# Phi example\n",
        "python scripts/osft_phi_example.py \\\n",
        "  --data-path /path/to/data.jsonl \\\n",
        "  --ckpt-output-dir /path/to/checkpoints \\\n",
        "  --unfreeze-rank-ratio 0.25\n",
        "```\n",
        "\n",
        "### When to Use OSFT vs SFT\n",
        "\n",
        "**Use OSFT when:**\n",
        "- Adding domain-specific knowledge to an already-trained model\n",
        "- Need to preserve original capabilities without regression\n",
        "- Don't have access to original training data for replay\n",
        "- Want to avoid catastrophic forgetting\n",
        "- Performing continual learning across multiple domains\n",
        "\n",
        "**Use SFT when:**\n",
        "- Training a model from scratch or base model\n",
        "- Have comprehensive training data covering all desired capabilities  \n",
        "- Don't need to preserve specific prior behaviors\n",
        "- Performing initial instruction tuning\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
